{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Workshop 4: What is a ChatGPT?\n","\n","- **When**: Monday Week 8, 17:00 - 18:30 \n","- **Where**: AT 5.04\n","- **Contact**: hello@edinburghai.org\n","- **Credits**: This notebook is created by EdinburghAI for use in its workshops. If you plan to use it, please credit us. \n","\n","## Today\n","\n","1. Understand what **word embeddings** are and how to use them for **semantic search**\n","2. **Fine-tune** a **pre-trained** model for sentiment classification of movie reviews ðŸŽ¬\n","3. Experiment with a **generative** model like ChatGPT ðŸ¤– \n","\n","Although not required, I would do Task 1 before Task 2. But you can do Task 3 completely separately. Task 1 has the most coding while Tasks 2 & 3 are mostly plug and play, with some questions to get you to think about what's going on. Feel free to jump around and do what interests you!\n","\n","Let's get started! ðŸ’¯"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Word Embeddings\n","\n","We've seen in the lecture that word embeddings are representations of word as a vector. This vector is generally pretty long - several hundred numbers long! This means the vectors live in a very high-dimensional space. In this high-dimensional space, different directions represent different meanings. One direction might represent 'capital city', and another might represent 'France' and when you add them together, you would get a meaning that corresponds to something like 'Paris'. And this really work as you'll see below!\n","\n","If you find the idea of a high-dimension space confusing, try and imagine it in just 2 or 3 dimensions. Each vector represents a direction, and different directions represent different meanings. This way, you can 'add' and 'subtract' meanings from each other. \n","\n","**ThinkðŸ¤”**: Why do you think we use several hundred dimensions instead of just 2 or 3? To help, imagine how many different directions you can represent in 1, 2 and 3D. What happens as you increases the number of dimensions. Now imagine hundreds of dimension! Even though meaning can be very nuanced, can you see that with enough dimensions, we might get somewhere?\n","\n","Let's see some word embeddings in action below!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Gensim is a library that contains some pre-trained word embeddings we can use\n","import gensim.downloader as api\n","\n","# It might take a while to download the model\n","model = 'glove-twitter-100'\n","word_vectors = api.load(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: How many words are in this model?\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Let's look at the embedding for the word 'artificial'\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: What is the dimensionality of the word vectors?\n","..."]},{"cell_type":"markdown","metadata":{},"source":["We can get the words that are closest to another using the `.most_similar()` method. Let's find the words that are closest to 'artificial'. Before running the next cell, predict a few words that you think should come up.\n","\n","Then go ahead and try out some other words!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Get the most similar words to 'artificial'. Then go ahead and try 'python', 'paris'\n","word_vectors.most_similar(...)"]},{"cell_type":"markdown","metadata":{},"source":["**ThinkðŸ¤”**: Was it what you expected? Remember how these vectors were trained! Can you see why we can't just naively use these embeddings in ChatGPT?\n","\n","We can also look at words that are most distant from a word by passing `negative='word'` to the `most_similar` method.\n","\n","**ThinkðŸ¤”**: What do you think will happen this time? Remember this was trained on twitter data, and remember how we trained the model? Then check it and try to reason through the answer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: We can look at words that are furthest from 'artificial'\n","word_vectors.most_similar(negative=...)"]},{"cell_type":"markdown","metadata":{},"source":["We can then do some maths with words using a combination of the `positive` and `negative` parameters of the `most_similar` method. Go ahead and get the most similar words for `'king - man + woman'` using the `most_similar` method. You can also try `'france - paris + tokyo'`. Or any word maths you like!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: We can look at words that are furthest from 'artificial'\n"]},{"cell_type":"markdown","metadata":{},"source":["**ExtensionðŸ˜ˆ**:\n","\n","Here is some code that tries to do the same as above but using a different method.\n","\n","```python\n","king, man, woman = word_vectors['king'], word_vectors['man'], word_vectors['woman']\n","\n","result = king - man + woman\n","\n","print(word_vectors.most_similar(result))\n","```\n","\n","It will work okay, but will return different numbers. Why do you think that is?\n","\n","*Hint: Go look up what most_similar does to its inputs when you supply multiple words. Or have a google of 'most_similar vs similar_by_vector'*\n","\n","Once you have an idea, how would you code it up so that the same numbers were returned?"]},{"cell_type":"markdown","metadata":{},"source":["### Cosine Similarity\n","\n","How is `most_similar` deciding which words are most similar? Well, we have two main options:\n","- Euclidean distance (Pythagoras' theorem for higher dimension)\n","- Cosine similarity (measure of angle between two vectors)\n","\n","You can use both, but generally we prefer cosine similarity as it measures the angle between 2 vectors. \n","\n","**ThinkðŸ¤”**: Why does it make sense that we'd rather measure angles than distances?\n","\n","*Hint: How is meaning represented in our embedding space?*\n","\n","Below, I've implemented cosine similarity for you - it's a pretty simple formula. You can look up 'angle between two vectors', you might even remember it from high school."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def cosine_similiarity(v1, v2):\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","# TODO: Let's calculate the cosine similarity between 'king' and 'queen'\n","..."]},{"cell_type":"markdown","metadata":{},"source":["You can check out [banana phone](https://bananaphone.web.app/) (made by Valentin who is part of EdinburghAI). It uses cosine similarity to decide whether a word is closest to the word 'banana' or 'phone'! \n","\n","**ExtensionðŸ˜ˆ**: Go and implement a version of banana phone below!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Implement banana phone! Only do this after you've tried the other exercises"]},{"cell_type":"markdown","metadata":{},"source":["## Semantic Search\n","\n","A simple way of searching through a bunch of documents using a search term like 'Why is Edinburgh University student satisfaction so low?' is to find the documents with words in common with you search. So it might find the documents that also have the terms 'Edinburgh University' and 'student satisfaction' in them. The ones with the most terms in common would be ranked at the top of our search.\n","\n","Instead, we're going to implement *semantic* search. We're now going to search our database of document trying to find the one that has the most similar *meaning*. So now we don't need exact matches. In the example above, it might also return results with terms like 'dissatisfied undergraduates' etc.\n","\n","To do this, we're going to need a *vector database*. In our case it's going to be pretty small - just a list. Then we're going to create a function that finds the most similar item and return it. Simple!\n","\n","Now, our documents are composed of many words. We could just add up embeddings, but we're going to cheat and use another package called `SentenceTransformer` that can create a vector for a whole sentence super easily. Here's an example."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You might need to install the package first.\n","%pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","# Load the model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Encode some sentences\n","sentences = ['This is an example sentence']\n","\n","sentence_embedding = model.encode(sentences)\n","print(sentence_embedding)"]},{"cell_type":"markdown","metadata":{},"source":["We can actually do more than one sentence at once."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentences = ['This is an example sentence', 'this is another sentence']\n","\n","# TODO: Encode the sentences and print the shape of the resulting tensor\n","sentence_embedding = ...\n","..."]},{"cell_type":"markdown","metadata":{},"source":["In the same way as before, we can look at the cosine similarity between two sentences. Try testing out different sentences and see what the model spits out!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentences = ['This is an example sentence', 'this is another sentence']\n","\n","# TODO: Calculate the cosine similarity between the two sentences\n","sentence_embedding = ...\n","..."]},{"cell_type":"markdown","metadata":{},"source":["And now we have a database (our list of sentence embeddings) and a way of measuring similarity. We have all we need to do semantic search!\n","\n","Here is a list of sentences:"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["sentences = [\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"Artificial intelligence is transforming industries worldwide.\",\n","    \"The history of the Roman Empire spans over a thousand years.\",\n","    \"Climate change is one of the most pressing issues of our time.\",\n","    \"Machine learning is a subset of artificial intelligence focused on data-driven predictions.\",\n","    \"The theory of relativity revolutionized modern physics.\",\n","    \"Quantum mechanics explores the behavior of matter at atomic and subatomic levels.\",\n","    \"Python is a popular programming language known for its simplicity and readability.\",\n","    \"The Great Wall of China is one of the Seven Wonders of the World.\",\n","    \"Blockchain technology underpins cryptocurrencies like Bitcoin.\",\n","    \"Astronomy is the scientific study of celestial objects and phenomena.\",\n","    \"The Industrial Revolution marked a major turning point in human history.\",\n","    \"Many animals use camouflage to blend into their environments.\",\n","    \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n","    \"Human rights are fundamental rights that every person is entitled to.\",\n","    \"The Amazon rainforest is home to an extraordinary diversity of species.\",\n","    \"E-commerce has changed the way consumers shop worldwide.\",\n","    \"Augmented reality overlays digital information onto the real world.\",\n","    \"Artificial neural networks are inspired by the human brain's structure.\",\n","    \"Renewable energy sources include solar, wind, and hydroelectric power.\",\n","    \"Shakespeare is widely regarded as one of the greatest writers in the English language.\",\n","    \"Nanotechnology is the science of manipulating matter on an atomic scale.\",\n","    \"Mental health awareness is increasingly recognized as essential to overall well-being.\",\n","    \"Globalization has led to increased interconnectedness between nations.\",\n","    \"Mars exploration has captured the interest of space agencies around the world.\",\n","    \"The Renaissance was a period of great cultural and artistic achievement in Europe.\",\n","    \"The Sahara Desert is the largest hot desert in the world.\",\n","    \"Meditation is a practice that can reduce stress and improve mental clarity.\",\n","    \"Blockchain offers a decentralized way to record transactions securely.\",\n","    \"The periodic table organizes elements by their atomic number.\",\n","    \"Mount Everest is the highest peak on Earth.\",\n","    \"Cryptography is essential for securing data in digital communications.\",\n","    \"Ocean acidification is a consequence of rising carbon dioxide levels.\",\n","    \"Social media platforms have transformed how people interact and share information.\",\n","    \"Data science involves extracting insights from complex data sets.\",\n","    \"The theory of evolution explains the diversity of life on Earth.\",\n","    \"Nutrition plays a critical role in maintaining health and wellness.\",\n","    \"The internet has dramatically changed access to information and knowledge.\",\n","    \"Anatomy is the study of the structure of organisms and their parts.\",\n","    \"The concept of democracy originated in ancient Greece.\",\n","    \"Microbiology studies organisms that are too small to be seen with the naked eye.\",\n","    \"Music has been an integral part of human culture for millennia.\",\n","    \"Space telescopes have greatly expanded our understanding of the universe.\",\n","    \"Genetics explores how traits are inherited through generations.\",\n","    \"The French Revolution was a pivotal event in European history.\",\n","    \"Coding skills are increasingly valuable in todayâ€™s job market.\",\n","    \"Marine biology focuses on life in ocean environments.\",\n","    \"Economics studies the production, distribution, and consumption of goods and services.\",\n","    \"Biomimicry uses nature-inspired solutions for human problems.\",\n","    \"The Milky Way is the galaxy that contains our solar system.\",\n","    \"Leadership skills are crucial for managing teams effectively.\",\n","    \"Renewable resources are vital for a sustainable future.\",\n","    \"Cybersecurity protects systems and networks from digital attacks.\",\n","    \"The human brain is a highly complex organ responsible for thought and behavior.\",\n","    \"Space travel requires advanced technology and rigorous training.\",\n","    \"The Cold War shaped international relations for much of the 20th century.\",\n","    \"Robotics involves the design, construction, and operation of robots.\",\n","    \"Philosophy seeks to answer fundamental questions about existence and knowledge.\",\n","    \"Biodiversity is essential for ecosystem resilience and stability.\",\n","    \"Journaling can be a powerful tool for self-reflection.\",\n","    \"Recycling helps reduce waste and conserve natural resources.\",\n","    \"The cardiovascular system circulates blood throughout the body.\",\n","    \"Inventions like the telephone and the internet transformed communication.\",\n","    \"Urbanization refers to the growth of cities as more people move to urban areas.\",\n","    \"Yoga combines physical postures, breathing exercises, and meditation.\",\n","    \"Photosynthesis takes place in the chloroplasts of plant cells.\",\n","    \"The Big Bang Theory describes the origin of the universe.\",\n","    \"Digital marketing has revolutionized how companies reach consumers.\",\n","    \"Volcanoes form when magma from beneath the Earthâ€™s crust reaches the surface.\",\n","    \"Exercise can improve physical health and mental well-being.\",\n","    \"The Mona Lisa is one of the most famous paintings in the world.\",\n","    \"Conservation efforts are critical for protecting endangered species.\",\n","    \"Sustainable agriculture aims to meet current food needs without harming the environment.\",\n","    \"Physics is the branch of science that studies matter, energy, and their interactions.\",\n","    \"Behavioral psychology examines the ways that people behave and learn.\",\n","    \"Cloud computing provides on-demand access to computing resources.\",\n","    \"Antibiotics are drugs that treat bacterial infections.\",\n","    \"Time management is an essential skill for productivity.\",\n","    \"Global warming refers to the long-term rise in Earth's average surface temperature.\",\n","    \"Vaccines help the immune system prevent disease.\",\n","    \"3D printing allows for the creation of complex physical objects from digital designs.\",\n","    \"Deep learning is a type of machine learning that uses neural networks with many layers.\",\n","    \"Ancient Egypt is known for its pyramids and pharaohs.\",\n","    \"Public speaking is a skill that can be improved with practice.\",\n","    \"Investing in stocks can be a way to build wealth over time.\",\n","    \"Ecology studies the interactions between organisms and their environment.\",\n","    \"Food security ensures that all people have access to sufficient food.\",\n","    \"Photosynthesis is the process by which plants convert light into energy.\",\n","    \"Social psychology explores how individuals are influenced by others.\",\n","    \"The immune system defends the body against harmful pathogens.\",\n","    \"Algebra is a branch of mathematics that deals with symbols and rules for manipulating them.\",\n","    \"Digital art allows artists to create work on computers and tablets.\",\n","    \"Astronauts undergo extensive training for missions in space.\",\n","    \"The respiratory system is responsible for taking in oxygen and expelling carbon dioxide.\",\n","    \"Entrepreneurship involves creating and managing a new business.\",\n","    \"Linguistics is the scientific study of language and its structure.\",\n","    \"Bird migration is a natural phenomenon involving long-distance travel.\",\n","    \"Virtual reality creates an immersive experience using computer technology.\",\n","    \"Hydroelectric power is generated by harnessing the energy of flowing water.\",\n","    \"In physics, gravity is the force that attracts two bodies toward each other.\",\n","    \"Self-driving cars use artificial intelligence to navigate roads.\"\n","]"]},{"cell_type":"markdown","metadata":{},"source":["Now, go and create your vector database."]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# TODO: Encode the sentences into a tensor we'll call our database. Print the shape of this tensor.\n","vector_db = ...\n","..."]},{"cell_type":"markdown","metadata":{},"source":["Now create a function that takes in a string, converts it to a vector, and queries the database for the most similar sentence."]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["# TODO: Implement the function that finds the most similar sentence to a given sentence\n","# Hint: You can use np.argmax to find the index of the most similar sentence\n","def find_most_similar(query: str) -> str:\n","    ..."]},{"cell_type":"markdown","metadata":{},"source":["And use it! Start by using one of the sentences in the database, then try changing eery word in the sentence but keeping the meaning the same."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Test the function by providing a query sentence and printing the most similar sentence\n","query_sentence = ...\n","\n","..."]},{"cell_type":"markdown","metadata":{},"source":["**ThinkðŸ¤”**: Do you understand how every part of the system works? How good is your system? What's the main problem? What would you change?\n","\n","**ExtensionðŸ˜ˆ**: Create a new system that encodes sentences using word2vec from above. However, word2vec is only able to produce an embedding for each word, so how will you encode an entire sentence? What do you think are the advantages/drawbacks of such a system compared to the sentence embeddings above?\n","\n","You can read more about how these sentence embeddings were created at [SentenceBert](https://sbert.net/) or the associated [paper](https://arxiv.org/abs/1908.10084).\n","\n","### Word Embeddings âœ…\n","\n","And that's word embeddings! Time to move on to language models!"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Fine-Tuning for Classification\n","Next, we are going to **fine-tune** a **pre-trained** language model on a task called sentiment analysis. This sort of thing is done a lot.\n","\n","In sentiment analysis, we want to assign a sentiment to a piece of text. This might be to characterise tweets as 'safe' vs 'harmful' or even 'truth' vs 'lie' (this is a tough one...). We're going to be categorising movie reviews into either 'positive' or 'negative'. It's just a kind of classification.\n","\n","**ThinkðŸ¤”**: How would you build a model that did this classification without any Machine Learning? Now think how you'd try to handle phrases like 'not ...' and 'lacking in ...'. Can you see why ML can be useful here?\n","\n","We are going to use a big pre-trained language model called [BERT](https://arxiv.org/pdf/1810.04805) which is able to produce **contextualised** word embeddings. Hopefully this means that those embeddings are more nuanced and rich in meaning than the ones we used above from word2vec. Then we'll use those those embeddings to decide whether or not a review is positive or negative.\n","\n","We're not going to be using Pytorch anymore! For many pre-trained models, they are stored on a webiste called [HuggingFace](https://huggingface.co/) (HF). HF also stores many datasets, and provides a python library for use to use all of these quite easily. The libraries we'll be using are [datasets](https://huggingface.co/docs/datasets/en/index) for datasets and [transformers](https://huggingface.co/docs/transformers/en/index) for the models. These libraries isn't a good choice for building your own model architecture, but it is very neat if you just want to use someone else's model.\n","\n","The first step is to load the dataset from huggingface, and build a training and testing split. You can run this cell without any changes."]},{"cell_type":"code","execution_count":88,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Load IMDB movie reviews\n","data = load_dataset('imdb')\n","train = data['train'].shuffle(seed=42).select(range(500))\n","test = data['test'].shuffle(seed=42).select(range(100))"]},{"cell_type":"markdown","metadata":{},"source":["Next, we want to **tokenize** our data. This means splitting up a sentence into smaller parts that are in the model's vocabulary. This is quite similar to splitting up a sentence into words, but can also include punctuation, other languages, and often models split up words into smaller chunks. You can check out a few different tokenizers for new and older versions of ChatGPT on [OpenAI's Platform](https://platform.openai.com/tokenizer). \n","\n","**ThinkðŸ¤”**: What do you notice about the difference in tokenization between the older and newer models. Why do you think that is?\n","\n","*Hint: If tokens are longer, then what will that mean about the size of the vocabulary? What might that mean about the model itself? Does this fit with what you might already know about versions of ChatGPT over the last few years?*\n","\n","BERT uses sub-word tokenization (tokens are not necessarily entire words, but pieces of words). We can just load the BERT *tokenizer* from huggingface."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# Create BERT tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","def tokenize_data(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n","\n","train_tokenized = train.map(tokenize_data, batched=True)\n","test_tokenized = test.map(tokenize_data, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["Have a poke around the `train_tokenized` object. It's a bit like a python dictionary with some attributes you can explore. \n","\n","*Hint: If you access the ['input_ids'] attribute, to save some time, you can instead access just the first one with '['input_ids'][0]'*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Look around the train_tokenized object. What do you see?\n","..."]},{"cell_type":"markdown","metadata":{},"source":["**ThinkðŸ¤”**: Do you understand what `text`, `label` and `input_ids` means?\n","\n","**ExtensionðŸ˜ˆ**: What do you think the `attention_mask` is? In our case, all of the values are `1`. If I told you that when training ChatGPT for next-token prediction, the attention mask has some `0`s that block out some of the attention between different words, can you have a guess at what it does? Have a google around or ask someone if you're interested.\n","\n","**ThinkðŸ¤”**: Above, we passed an argument called `padding=max_length`, and `truncation=True` to the tokenizer. Given that each training sample needs to be a fixed length for passing into our model, what might padding and truncation do to our reviews?\n","\n","Now we are ready to fine-tune our model. First, we load a pre-trained BERT model (this saves us having to train a whole model from scratch). The model we are using is specifically catered for classification. We choose to give it 2 output labels (**ThinkðŸ¤”**: What are our two output labels?). \n","\n","Next, we will setup a **Trainer** with some **TrainingArguments**. Don't worry too much about these, they're just a bunch of boilerplate provided by HF that save us from having to write our own training code."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n","\n","# We give the model a classification head\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\", # Where we store our results\n","    eval_strategy=\"epoch\", # How often we evaluate model (every epoch)\n","    learning_rate=2e-5, # Learning rate\n","    per_device_train_batch_size=8, \n","    per_device_eval_batch_size=16, \n","    num_train_epochs=2, # How many epochs\n","    weight_decay=0.01, # This prevents our model from overfitting\n","    report_to=\"none\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tokenized,\n","    eval_dataset=test_tokenized,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**ExtensionðŸ˜ˆ**: Go and google 'weight decay in nn'. Also google 'learning rate scheduler'. Why do you think these are useful?"]},{"cell_type":"markdown","metadata":{},"source":["We are finally ready to train our model! - which is trivially (almost laughably) easy with HF. Note that this can take about 5-6 mins to run fully on Kaggle."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["Now that our model is fine-tuned, we evaluate it on our test data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = trainer.evaluate()\n","print(metrics)"]},{"cell_type":"markdown","metadata":{},"source":["Great! Having these numbers is all well and good, but they probably don't mean anything to you at the moment. You should definitely also calculate the accuracy of the model on your test set, but we're going to skip that here. Instead, to get a better idea of how our model works, we're going to write a sentence or two in the `review` field of the following cell, and see if the model correctly predicts whether it is a `Positive` or `Negative` review of a film."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","\n","# TODO: Provide a test movie review\n","review = ...\n","\n","# TODO: Try to understand what this function does\n","def predict_sentiment(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n","    outputs = model(**inputs)\n","    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n","    return \"Positive\" if predicted_class == 1 else \"Negative\"\n","\n","# TODO: Test the function\n","..."]},{"cell_type":"markdown","metadata":{},"source":["**ThinkðŸ¤”**: Are there any situations that this model incorrectly predicts? (Hint: try phrases like \"not good\" or \"not bad\".) How might you improve this model to overcome such shortcomings?\n","\n","**Optional**: If you want, run the following code to train the model on a larger training set, then run the above cell again to see if your results have improved! (This is optional because it will take a while)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This cell is optional and may take a while to run! (15-20 mins)\n","train_large = data['train'].shuffle(seed=42).select(range(1500))\n","test_large = data['test'].shuffle(seed=42).select(range(250))\n","\n","encoded_train_large = train_large.map(tokenize_data, batched=True)\n","encoded_test_large = test_large.map(tokenize_data, batched=True)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_train_large,\n","    eval_dataset=encoded_test_large,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["#### Text Classification âœ…\n","\n","# Part 3: Using a GPT\n","\n","Next up, we are going to use a pre-trained model to generate some text! For this task we will use the GPT-2 model, which is the predecessor of ChatGPT's current models, GPT-3.5 and GPT-4. It came out in 2019 - so you'll really be able to see how the models have improved since then. It's stupidly easy to get up and running. Go ahead and run the next two cells to set up our GPT model and text generation function."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# TODO: Load GPT-2 model and tokenizer. The model's name is 'gpt2'. You can use the same format as above ('.from_pretrained'), no extra arguments needed. \n","tokenizer = ...\n","model = ...\n","\n","# Ignore this\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# TODO: Try to understand what this function does\n","def generate_text(prompt, max_length=50, temperature=1.0, top_k=0, top_p=1.0):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.generate(\n","        inputs.input_ids,         \n","        attention_mask=inputs.attention_mask,\n","        pad_token_id=tokenizer.eos_token_id,\n","        max_length=max_length,\n","        temperature=temperature,  # Controls randomness\n","        top_k=top_k,              # Limits to top-k most likely words\n","        top_p=top_p,              # Nucleus sampling\n","        do_sample=True            # Enables sampling for non-deterministic results  \n","    )\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now we are ready to start generating text! \n","\n","The output of a generative text model is a vector of scores for each output token/word. A simple way of choosing which token/word should go next is to choose the biggest score, but as you'll see, this isn't the ideal behaviour (**ThinkðŸ¤”**: Do you always say the most probable word next?). This vector of scores is actually a probability distribution that sums to 1. We could then just sample from this distribution (**ThinkðŸ¤”**: As a concrete example, what would sampling from the distribution [1/6, 1/6, 1/6, 1/6, 1/6, 1/6] look like?) We can manually edit this probability distribution to change how the model generates text.\n","\n","**ExtensionðŸ˜ˆ**: You can google 'softmax' to see how this happens we make this probability distribution. TLDR: the model does output a bunch of scores that could be any numbers, and then we squish them into [0,1] and make sure they add up to 1. This operation still makes sure that the highest scores have the highest probabilites, and does this in a sensible way.\n","\n","Here's a quick overview of what each one does. As a running example, let's pretend we only have 3 words and the model has outputted the distribution: `[0.8, 0.15, 0.05]` i.e. it think the word with id `0` is most likely.\n","\n","- `temperature`: Temperature controls how much we change this distribution manually in order to change the kind of text generated. Higher temperature values make the scores closer together, making the probability of selecting each word more similar, leading to a more creative model. However, if temperature is too high, the model can start to make little sense - have a go! Conversely, lower temperature values make the model **less** random, with values close to 0 leading to a nearly deterministic model. Try out different values yourself and see where you think a good range of temperatures is.\n","E.g. Suppose our distribution was `[0.8, 0.15, 0.05]`. Then a high temperature would turn this into `[0.6, 0.25, 0.15]` while a lower temperature would so something like `[0.95, 0.04, 0.01]`.\n","- `top_k`: Top-k sampling is a technique that limits the model's output to the top-k most probable words. This helps to keep the model coherent, but restricts the creativity and randomness of the model. Try some small values, like 5-10, and compare your results to bigger values, like 50 or 100.\n","\n","E.g. `top_k = 2` would yield `[0.8, 0.15]` - except this doesn't add to 1 so we normalise such they add to 1, but the first element is still 8 times as likely as the first: `[0.842, 0.158]`.\n","- `top_p`: Similar to `top_k`, this is another sampling technique (also known as nucleus sampling). Basically, the model chooses a subset of the most probable words such that their cumulative probability is at least `p` (e.g. 0.9). This prevents words with a very low probability from being selected, while maintaining some room for randomness.\n","\n","e.g. `top_p = 0.95` gives `[0.8, 0.15]` which we would normalise as above. Similarly,  `top_p = 0.8` would give `[0.8]` which we can normalise as before. **ThinkðŸ¤”**: What would it normalise to? *Hint: There's only 1 option!* \n","\n","#### Have a go!\n","\n","In the cell below, try messing around with the below values and see how the output changes! Also, try changing the input and the `max_length` (although if you set it to be much more than 200 it's going to take a while...)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompt = \"Complete this story: \\n In a distant future, artificial intelligence has become\"\n","\n","temperature = ... # Choose a float > 0 (must have a decimal point)\n","top_k = ... # Choose any positive integer (0 does nothing)\n","top_p = ... # Choose any float between 0 and 1 (1 does nothing)\n","\n","print(generate_text(prompt, max_length=50, temperature=temperature, top_k=top_k, top_p=top_p))"]},{"cell_type":"markdown","metadata":{},"source":["**ThinkðŸ¤”**: Do you think we want deterministic outputs? Why or why not? There are 2 different ways above to set the model to be deterministic (If we were allowed to set `temperature = 0`, there would even be 3). Check that they all produce the same (there might be tiny variations for unimportant reasons, but in most cases it should be exactly the same). Do you like the kind of text it produces? Why do you think it exhibits this behaviour?\n","\n","**ExtensionðŸ˜ˆ**: There are other ways generate text using such a model. Go and google 'beam search for generative language models'. Why do you think this might be better? See if you can work out from the HF docs for the `generate` method how to get it working.\n","\n","**ExtensionðŸ˜ˆ**: One of the big problems with transformer models like the ones above is that the computation requires scales quadratically with the length of the input. This is why if you set `max_length` to be twice as large, it more than doubles the computation time. Can you think about *why* the scaling is quadratic? Remember that that transformer uses attention under the hood. Ask someone to have a chat about it.\n","\n","The simplest way to query a model is using ChatGPT's website. However, you can do the same thing in code using the [OpenAI API](https://pypi.org/project/openai/) or packages like [langchain](https://www.langchain.com/). We didn't use those here because we want to use our own models, but often times, you won't be able to make a better model than ChatGPT - so just user their API!\n","\n","#### ChatGPT âœ…\n","\n","# Language Models âœ…\n","> That's it! I hope you see that it's not so hard to get building with some of the most powerful models in the world right now. You could even start a business around it (lots of people are) - the so-called 'GPT-wrapper' companies. If you do it well, you might make a lot of money. \n",">\n","> On the flipside, I hope you agree how cool word embeddings are, and also how powerful they are. There's still **loads** of active research in NLP (Natural Language Processing), trying to take most advantage of these new models we've built in just the last few years, or trying to create models that are less stupid and more aligned to what humans want.\n",">\n","> Go learn more and build something cool. Then come back and show us!\n","> See you soon! \n","\n","Pierre Mackenzie & Niall Meagher, Edinburgh AI\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"edinburgh-ai-workshops","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
