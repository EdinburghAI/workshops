{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2: Using Computer Vision for Good! üöÄ\n",
    "\n",
    "- **When:** Wednesday, February 26th, 17:00.\n",
    "- **Where:** Appleton Tower, 5.04. If you don't have access, reach out and we'll let you in!\n",
    "- **Contacts:** Reach out on Instagram _@edinburgh.ai_\n",
    "- **Credits:** This notebook is created by EdinburghAI for use in its workshops. If you plan to use it, please credit us!\n",
    "\n",
    "## Today\n",
    "- Today we're building a Sign Language Interpreter. \n",
    "- We'll take hundreds of photos of different hand signs and extract the relevant information.\n",
    "- We'll try training different models.\n",
    "- We'll then upload our own hand sign and see if our model accurately predicts it!\n",
    "\n",
    "Lfg üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "## IMPORTANT! Turn On Internet\n",
    "1. On the right-hand side of this notebook, there's a section called **\"Session Options\"**.\n",
    "2. Scroll down to the _\"Internet\"_ toggle. Turn it on. You may need to verify your phone number.\n",
    "3. Additionally, to help this run faster, you can also enable some GPU access.\n",
    "\n",
    "\n",
    "## Using Jupyter:\n",
    "This is a Jupyter notebook. It contains cells. There are 2 kinds of cells - markdown and Python. Markdown cells are like this one, and are just there to give you information. Python cells run code. You can run a cell with `Ctrl + Enter` or run a cell and move the next one with `Shift + Enter`. Try running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:11.029407Z",
     "iopub.status.busy": "2025-02-26T12:23:11.029063Z",
     "iopub.status.idle": "2025-02-26T12:23:11.036355Z",
     "shell.execute_reply": "2025-02-26T12:23:11.035248Z",
     "shell.execute_reply.started": "2025-02-26T12:23:11.029380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Ctrl + Enter runs this cell!')\n",
    "output = 'The last line of a cell is printed by default'\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:11.037935Z",
     "iopub.status.busy": "2025-02-26T12:23:11.037612Z",
     "iopub.status.idle": "2025-02-26T12:23:42.703837Z",
     "shell.execute_reply": "2025-02-26T12:23:42.703123Z",
     "shell.execute_reply.started": "2025-02-26T12:23:11.037901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install mediapipe\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import mediapipe as mp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle  # For model saving & loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.keras import TqdmCallback  # Import progress bar for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Our Data\n",
    "\n",
    "Like all good AI projects, we need to explore our data first. Check it out! (Check the OpenCV [docs](https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#ga384030628244491e687668b7da7cac94).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:42.705729Z",
     "iopub.status.busy": "2025-02-26T12:23:42.705206Z",
     "iopub.status.idle": "2025-02-26T12:23:43.073409Z",
     "shell.execute_reply": "2025-02-26T12:23:43.072472Z",
     "shell.execute_reply.started": "2025-02-26T12:23:42.705702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We'll be using OpenCV for this project, a popular library for computer vision.\n",
    "import cv2\n",
    "\n",
    "# TODO: Let's load in an image and display it (using openCV imread ideally...).\n",
    "example_image = ... # Image path...\n",
    "img = ...\n",
    "plt.imshow(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm. That doesn't look quite right. Turns out OpenCV defaults to BlueGreenRed, but our eyes do not lol. \n",
    "\n",
    "ü§î _Why do you think that is?_ \n",
    "\n",
    "Regardless, let's convert the BGR image to an RGB image. _(Again, check the [docs](https://www.geeksforgeeks.org/python-opencv-cv2-cvtcolor-method/)!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:43.074787Z",
     "iopub.status.busy": "2025-02-26T12:23:43.074523Z",
     "iopub.status.idle": "2025-02-26T12:23:43.328567Z",
     "shell.execute_reply": "2025-02-26T12:23:43.327742Z",
     "shell.execute_reply.started": "2025-02-26T12:23:43.074766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Convert the BGR image to an RGB one.\n",
    "img_rgb = ...\n",
    "plt.imshow(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's better ü§ü! \n",
    "\n",
    "Ok sweet. Now that we have an idea of how the images look, we're going to use them to train two different models. \n",
    "- For the first model, we're going to take the crucial joints of the hand (wrist, phalanx etc. ‚úã) and feed _those_ into a _simple_ model.\n",
    "\n",
    "- For the second, we're just going to say screw it and feed the entire image into a Convolutional Neural Network and pray. We'll not go into how it works today, but if you're curious, go to our [workshop](https://github.com/EdinburghAI/workshops/blob/main/Sem1Workshop3/CNNs-Solved.ipynb) from semester 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Key Features of the Hand\n",
    "\n",
    "#### Ok... How?\n",
    "We're gonna use a library called _MediaPipe_ üëÄ. It's a suite of models designed for use in Computer Vision ü§ñ. We're going to use their _Hands_ model to segment the hands into its parts. If it detects there's hands in the photo, it'll place its parts as coordinates on top of the photo üîé. \n",
    "\n",
    "Let's try it out!\n",
    "\n",
    "_Stuck? There's some helpful demo's online. If you can't find them, try this [one](https://medium.com/@amineouahidialaoui/build-your-own-hand-detector-with-python-in-seconds-6c8ddb486d7d) üòâ._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:43.329794Z",
     "iopub.status.busy": "2025-02-26T12:23:43.329484Z",
     "iopub.status.idle": "2025-02-26T12:23:43.689410Z",
     "shell.execute_reply": "2025-02-26T12:23:43.688561Z",
     "shell.execute_reply.started": "2025-02-26T12:23:43.329772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We're going to use the hands library. \n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# We want to now initialise the Hands class. Keep in mind, \n",
    "# we'll using it on a static image (ie static_image_mode) \n",
    "# and will want to be reasonably confident (at least 30%)\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# TODO: Process the RGB photo\n",
    "hand_landmarks = ...\n",
    "\n",
    "# We're going to iterate over each landmark and draw it.\n",
    "for landmark in hand_landmarks.multi_hand_landmarks:\n",
    "\n",
    "    #¬†We could do with the drawing_utils' draw_landmarks function right about now.\n",
    "    mp_drawing.draw_landmarks(\n",
    "            ...,  # TODO: image to draw on\n",
    "            ...,  # TODO: Each individual landmark of the hand\n",
    "            mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "    plt.imshow(...) # TODO: Plot the RGB image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusability!\n",
    "We're going to be loading an image, and turning it into a landmark a lot. Let's make it simpler by making it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:43.690848Z",
     "iopub.status.busy": "2025-02-26T12:23:43.690493Z",
     "iopub.status.idle": "2025-02-26T12:23:43.701639Z",
     "shell.execute_reply": "2025-02-26T12:23:43.700318Z",
     "shell.execute_reply.started": "2025-02-26T12:23:43.690818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the Hands class\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "def take_image_path_and_return_landmarks(img_path):\n",
    "    # TODO: Read the image using OpenCV (same as above!)\n",
    "    img = ...\n",
    "    \n",
    "    # TODO: Turn the image into RGB\n",
    "    img_rgb = ...\n",
    "    \n",
    "    # Process the RGB image\n",
    "    hand_landmarks = ...\n",
    "    \n",
    "    return hand_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great. Now get those landmarks for every image.\n",
    "Sweet! So we've found the landmarks of a single hand image using `mp.solutions.hands`. Now, let's do some Python to get that of every hand image.\n",
    "\n",
    "\n",
    "#### Normalising\n",
    "Take a look at the photo above. The coordinates are all roughly in the middle of the image. But what if all of the images of `B` were in the top left corner. The model might learn that if the coordinates are all in the top left corner, then it's `B` (üëé). \n",
    "\n",
    "So how do we fix that? Well what if we subtract all of the coordinates by a certain amount, so they're always in the bottom left... This is called _normalisation_. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:23:43.703509Z",
     "iopub.status.busy": "2025-02-26T12:23:43.703142Z",
     "iopub.status.idle": "2025-02-26T12:24:27.137323Z",
     "shell.execute_reply": "2025-02-26T12:24:27.136545Z",
     "shell.execute_reply.started": "2025-02-26T12:23:43.703473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Where we're storing the data. If you've got any problems with it, try messing around with this part. \n",
    "DATA_DIR = \"/kaggle/input/synthetic-asl-alphabet/Train_Alphabet\"\n",
    "\n",
    "def turn_all_images_to_landmarks(DATA_DIR):\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    # We want to iterate over the entire training folder. \n",
    "    for dir_ in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        # Iterate over every single letter in each letter's folder. \n",
    "        # We're only going to take the 30 images, it's incredibly slow otherwise.\n",
    "        for img_path in os.listdir(os.path.join(DATA_DIR, dir_))[:30]:\n",
    "\n",
    "            # We'll store our coordinates here, temporarily, as we normalise them.\n",
    "            x_ = []\n",
    "            y_ = []\n",
    "\n",
    "            # Same as before...\n",
    "            # TODO: Take image and return the landmark. If only we had a function for that.\n",
    "            individual_photo_path = os.path.join(DATA_DIR, dir_, img_path)\n",
    "            results = ...\n",
    "\n",
    "            \n",
    "            # Check if there was a hand discovered in the first place.\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Only take the first hand (our model doesn't know deal with 2 hands)\n",
    "                hand_landmarks = results.multi_hand_landmarks[0]  \n",
    "                \n",
    "                # Take every x coordinate for every landmark\n",
    "                x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "                \n",
    "                # Take every y coordinate for every landmark\n",
    "                y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "                \n",
    "                data_aux = []\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    # This normalises them to be all centered at 0.\n",
    "                    data_aux.append(x_[i] - min(x_))\n",
    "                    data_aux.append(y_[i] - min(y_))\n",
    "                data.append(data_aux)\n",
    "                labels.append(dir_)\n",
    "\n",
    "        print(f\"Finished letter {dir_}\")\n",
    "    return labels, data\n",
    "    \n",
    "\n",
    "\n",
    "labels, data = turn_all_images_to_landmarks(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ummm. Did that work? Let's take a look at the what we made..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:24:27.139262Z",
     "iopub.status.busy": "2025-02-26T12:24:27.139047Z",
     "iopub.status.idle": "2025-02-26T12:24:27.144777Z",
     "shell.execute_reply": "2025-02-26T12:24:27.144032Z",
     "shell.execute_reply.started": "2025-02-26T12:24:27.139244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Let's print out ONLY the zeroeth data entry and zeroeth label entry.\n",
    "# Don't try print the entire thing. You'll crash your browser lol. There's a lot of floats.\n",
    "print(...)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweet! That seems right...\n",
    "Basically, they're a list of coordinates (normalised between 0-1), followed by the classification of the letter _\"N\"_. \n",
    "\n",
    "Now let's train a model. Let's use RandomForest. Don't forget to split the data into a corresponding train-test split and then get the accuracy. \n",
    "\n",
    "Why use not use a neural network? Well we'll get to that, always try the simplest solution first and build up from there. RandomForest is incredibly efficient. \n",
    "\n",
    "If all of that seems like non-sense, take a look at our first workshop we went [through in Sem1](https://github.com/EdinburghAI/workshops/blob/main/Sem1Workshop1/IntroToML/IntroToML-Solved.ipynb) - Scroll down to _\"Decision Trees\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:24:27.146047Z",
     "iopub.status.busy": "2025-02-26T12:24:27.145847Z",
     "iopub.status.idle": "2025-02-26T12:24:27.509964Z",
     "shell.execute_reply": "2025-02-26T12:24:27.509250Z",
     "shell.execute_reply.started": "2025-02-26T12:24:27.146031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: We'll split our data into a train-test  split. \n",
    "# This technically means we're not using the testing part of the dataset but fuck it we ball. \n",
    "x_train, x_test, y_train, y_test = ...\n",
    "\n",
    "# Create and train a RandomForestModel with sklearn (If unsure, check Sklearn RandomForests Docs - They're great!)\n",
    "# Try different models as well! RandomForest isn't the one with the best performance üëÄ\n",
    "model = ...()\n",
    "... # TODO: Train on x training and y training set. \n",
    "\n",
    "# TODO: Let's predict all of the the outputs for the x_testing set. \n",
    "y_predict = ...\n",
    "\n",
    "# TODO: Check the accuracy score for our predictions by comparing the y_predict to y_test. \n",
    "score = ...\n",
    "print('{}% of samples were classified correctly!'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That was fast! \n",
    "The model trained reallyyyy quickly. In ML, you often don't have to pick the fanciest model. \n",
    "\n",
    "We're basically taking coordinates and trying to find a shape between them - therefore our model doesn't need to be the most complicated. This makes everything wayyy faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Time\n",
    "Now let's test it out! Take a photo on your phone of you doing a Sign and see if the model will recognise it!\n",
    "\n",
    "\n",
    "To do this:\n",
    "- Take a photo on your phone. Ensure there's only 1 hand visible in the frame.\n",
    "- Pass the photo to your laptop.\n",
    "- On the right hand side, scroll to the _\"Upload\"_ button > _\"New Dataset\"_ > File.\n",
    "- Drag and drop your JPG file. \n",
    "- Give it a name\n",
    "- Press create!\n",
    "- Once loaded, copy the exact location of the file and paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:25:36.382663Z",
     "iopub.status.busy": "2025-02-26T12:25:36.382303Z",
     "iopub.status.idle": "2025-02-26T12:25:36.588966Z",
     "shell.execute_reply": "2025-02-26T12:25:36.587981Z",
     "shell.execute_reply.started": "2025-02-26T12:25:36.382636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# üöÄ Inference Function\n",
    "# -------------------------\n",
    "def infer_single_image_using_solution_1(img_path):\n",
    "    \"\"\"\n",
    "    Takes an image path, extracts hand landmarks, and predicts the ASL letter using the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess image. Again, a function would be convenient. \n",
    "    results = take_image_path_and_return_landmarks(img_path)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]  # Only take the first detected hand\n",
    "        x_ = np.array([lm.x for lm in hand_landmarks.landmark])\n",
    "        y_ = np.array([lm.y for lm in hand_landmarks.landmark])\n",
    "\n",
    "        # ‚úÖ Normalize features (same as training)\n",
    "        x_min, y_min = x_.min(), y_.min()\n",
    "        data_aux = np.column_stack((x_ - x_min, y_ - y_min)).flatten().tolist()\n",
    "\n",
    "        # ‚úÖ Predict using trained model\n",
    "        prediction = model.predict([data_aux])[0]\n",
    "\n",
    "        return prediction  # Return predicted label\n",
    "    else:\n",
    "        return \"No hand detected!\"\n",
    "\n",
    "def draw_and_predict(img_path):\n",
    "    # use our above function to map the hand\n",
    "    landmarks = take_image_path_and_return_landmarks(img_path)\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # once again convert to RGB\n",
    "\n",
    "    # now we want to visualise our hand!\n",
    "    for landmark in landmarks.multi_hand_landmarks:\n",
    "        \n",
    "        ## -- this is just for looks to thicken the lines drawn --\n",
    "        custom_landmark_style = mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "        custom_connection_style = mp_drawing_styles.get_default_hand_connections_style()\n",
    "\n",
    "        for connection_style in custom_connection_style.values():\n",
    "            connection_style.thickness = 10\n",
    "        for landmark_style in custom_landmark_style.values():\n",
    "            landmark_style.thickness = 10\n",
    "            landmark_style.circle_radius = 5\n",
    "        mp_drawing.draw_landmarks(\n",
    "                image_rgb,  # image to on\n",
    "                landmark,  # the landmarks for the hand\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "        \n",
    "    # and predict the Letter using our infer function\n",
    "    predicted_label = infer_single_image_using_solution_1(img_path)\n",
    "    # RETURN OUR PREDICTION AND IMAGE!\n",
    "    return predicted_label, image_rgb\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# üöÄ Example Usage\n",
    "# -------------------------\n",
    "img_path = \"/kaggle/input/another/IMG_4193.JPG\"  # Replace with your actual test image\n",
    "predicted_label, image = draw_and_predict(img_path)\n",
    "plt.imshow(image)\n",
    "print(f\"Predicted ASL Letter: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Solution 2 üß†:\n",
    "\n",
    "Sweet! Well our solution 1 seems to work incredibly well. Let's try out solution 2. \n",
    "\n",
    "Solution 2 is _significantly_ more computationally expensive. I'd _highly_ recommend you pop on the GPU's if you want it to finish training for this workshop lol. \n",
    "\n",
    "Also, for the first solution, we only used ~1/3 of the data available, and we _still_ performed really well. Now, in order for solution 2 to do even half-way decent, we need *all* the data. This will take longer to prepare.\n",
    "\n",
    "#### Preparing The Data\n",
    "\n",
    "Aside from GPUs, we also need to change the images slightly so they'll fit into our CNN. For a refresher on CNNs, check out our [semester 1 workshop](https://github.com/EdinburghAI/workshops/blob/main/Sem1Workshop3/CNNs-Solved.ipynb) on them! _TLDR: They're neural networks that're specialised for images._\n",
    "\n",
    "How're we going to standardise the data? We'll need to turn their colours to be between 0 and 1 (where each RGB is currently between 0-255). We'll also need to resize their pixels to be 128x128. \n",
    "\n",
    "Why for both? Well we're going to be feeding each pixel into the network. The model will perform better if the values are normalised to be between 0 and 1. \n",
    "\n",
    "And why resize to 128x128? Kind of arbitrary, but it's a smaller size (which allows it to train faster as there's less calculations). Also, the square shape makes the model architecture simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:26:15.402532Z",
     "iopub.status.busy": "2025-02-26T12:26:15.402219Z",
     "iopub.status.idle": "2025-02-26T12:26:16.063118Z",
     "shell.execute_reply": "2025-02-26T12:26:16.062171Z",
     "shell.execute_reply.started": "2025-02-26T12:26:15.402507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def utilise_gpus(): \n",
    "    # Check for available GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"‚úÖ Using {len(gpus)} GPUs\")\n",
    "        for gpu in gpus:\n",
    "            # Tell tensorflow to use the GPU's\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"‚ùå No GPU detected, training on CPU.\")\n",
    "\n",
    "utilise_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:29:09.202295Z",
     "iopub.status.busy": "2025-02-26T12:29:09.202001Z",
     "iopub.status.idle": "2025-02-26T12:29:09.207903Z",
     "shell.execute_reply": "2025-02-26T12:29:09.207045Z",
     "shell.execute_reply.started": "2025-02-26T12:29:09.202274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Data from Raw Images\n",
    "def load_images(DATA_DIR, IMG_SIZE):\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    # Iterate over every image, make a dictionary that gives it its label.\n",
    "    label_map = {label: idx for idx, label in enumerate(os.listdir(DATA_DIR))}\n",
    "\n",
    "    for label in tqdm(os.listdir(DATA_DIR)):\n",
    "        # Iteratve over every individual image\n",
    "        for img_path in os.listdir(os.path.join(DATA_DIR, label)):\n",
    "            \n",
    "            # Take the image, turn it to RGB (like before) and resize it.\n",
    "            img = cv2.imread(os.path.join(DATA_DIR, label, img_path))\n",
    "            img = ... # TODO: Convert to RGB\n",
    "            img = ... # TODO: Resize the image to 128x128 using the cv2.resize functionality...\n",
    "\n",
    "            # Add this image to our data list\n",
    "            data.append(...)\n",
    "            labels.append(label_map[label])\n",
    "    \n",
    "    # Normalise colours from 0-255 to 0-1.\n",
    "    data = np.array(data, dtype=\"float32\") / 255.0  \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:29:10.464306Z",
     "iopub.status.busy": "2025-02-26T12:29:10.463997Z",
     "iopub.status.idle": "2025-02-26T12:38:51.347288Z",
     "shell.execute_reply": "2025-02-26T12:38:51.346334Z",
     "shell.execute_reply.started": "2025-02-26T12:29:10.464280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Constants\n",
    "IMG_SIZE = 128  # Resize all images to 128x128\n",
    "BATCH_SIZE = 64  # Adjustable based on GPU memory\n",
    "EPOCHS = 10\n",
    "DATA_DIR = \"/kaggle/input/synthetic-asl-alphabet/Train_Alphabet\"\n",
    "\n",
    "# Load dataset\n",
    "data, labels = load_images(DATA_DIR, IMG_SIZE)\n",
    "\n",
    "# How many different letters can we represent. \n",
    "# Hint: The correct answer is 27...\n",
    "num_classes = len(set(labels))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Convert labels to one-hot encoding using Keras to_categorical. (Google One-Hot encoding!)\n",
    "labels = to_categorical(..., ...)\n",
    "\n",
    "# TODO: Split data into train and test sets \n",
    "x_train, x_test, y_train, y_test = ...\n",
    "\n",
    "# Define the model to distribute the workload between GPUs.\n",
    "with tf.distribute.MirroredStrategy().scope():\n",
    "    \n",
    "    # Define the model. Don't worry about the specifics for this workshop, they were discussed in the Sem1Workshop3\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)), \n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile Model with multi-GPU strategy\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model with Progress Bar\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,  # Hide default logs\n",
    "                    callbacks=[TqdmCallback()])  # Show tqdm progress bar\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:43:42.753608Z",
     "iopub.status.busy": "2025-02-26T12:43:42.753185Z",
     "iopub.status.idle": "2025-02-26T12:43:43.162534Z",
     "shell.execute_reply": "2025-02-26T12:43:43.161843Z",
     "shell.execute_reply.started": "2025-02-26T12:43:42.753556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 128  # Must match training size\n",
    "DATA_DIR = \"/kaggle/input/synthetic-asl-alphabet/Train_Alphabet\"  # Update this with your actual dataset path\n",
    "\n",
    "\n",
    "# Create label mapping (same as in training)\n",
    "label_map = {label: idx for idx, label in enumerate(os.listdir(DATA_DIR))}\n",
    "reverse_label_map = {idx: label for label, idx in label_map.items()}  # Reverse lookup\n",
    "\n",
    "# -------------------------\n",
    "# üöÄ Inference Function\n",
    "# -------------------------\n",
    "def infer_single_image_using_cnn(img_path):\n",
    "    \"\"\"\n",
    "    Takes an image path, preprocesses it, and predicts the ASL letter using the trained CNN model.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image (same as before).\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize to match training input size\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize pixel values\n",
    "\n",
    "    # Expand dimensions to match CNN input shape (batch_size, height, width, channels)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # TODO: Make prediction on the image using the model.\n",
    "    predictions = ...\n",
    "    predicted_label_idx = np.argmax(predictions)  # Get index of highest probability\n",
    "\n",
    "    # Map index back to ASL letter\n",
    "    predicted_label = reverse_label_map[predicted_label_idx]\n",
    "    \n",
    "    return predicted_label  # Return predicted ASL letter\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# üöÄ Example Usage\n",
    "# -------------------------\n",
    "img_path = \"/kaggle/input/another/IMG_4193.JPG\"  # Replace with your test image path\n",
    "predicted_label = infer_single_image_using_cnn(img_path)\n",
    "print(f\"Predicted ASL Letter: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2275641,
     "sourceId": 3821116,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
