{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 3: Computer Vision with CNNs üëÄ\n",
    "\n",
    "- **When**: Monday Week 6, 17:00 - 18:30 \n",
    "- **Where**: AT 5.04\n",
    "- **Contact**: hello@edinburghai.org\n",
    "- **Credits**: This notebook is created by EdinburghAI for use in its workshops. If you plan to use it, please credit us. \n",
    "\n",
    "## Today\n",
    "1. Understand what a **convolutional layer** is and its configurable parameters.\n",
    "2. Train your own image classifier for satellite data! üñºÔ∏è \n",
    "3. Facial segmentationü•∏ of *your own face* with a pre-trained model. This one is fun and shouldn't take very long!\n",
    "\n",
    "Let's get started! üíØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Convolutional Layer\n",
    "\n",
    "A convolutional layer is like a double for loop ‚ûø over an image. At every location in the image üñºÔ∏è, it tries to detect some feature, like whether there is a vertical edge there, or if there's a blob of red üü•, or if there's a square there or lots of other sorts of things. At the end, we get a new 'image' with big values where there is an edge, and smaller values where there isn't.\n",
    "\n",
    "It does this using a *kernel*. The kernel of the convolution is the part that decides whether or not the feature is there. Really, the kernel is just another *tensor* (remember what a tensor is?). For grayscale images, we normally think of a kernel as being 2D, which is nice because they're easy to visualise! Generally, our kernels are square of size $n\\times n$, where we can decide how big they should be.\n",
    "\n",
    "You can learn why it's called a *convolution* from 3Blue1Brown [here](https://www.youtube.com/watch?v=KuXjwB4LzSA). TLDR: It's a mathematical operation where you slide one set of numbers (.e.g. our image) over another (e.g. our *kernel*), cross-multiply and add up the results.\n",
    "\n",
    "**Think**ü§î: What do you think the dimensionality of a kernel that takes an RGB image as input is? **Extensionüòà**: What about a kernel that takes some time-series data as input?\n",
    "\n",
    "**Think** ü§î: What do you think the following kernels do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = [[-1, 0, 1],\n",
    "           [-1, 0, 1],\n",
    "           [-1, 0, 1]]\n",
    "\n",
    "# Why do you think I've set the middle column to be 2s in the example below? (What do you think the kernel is trying to do? What would happen if they were 1s and you ran this kernel over a uniform wall?)\n",
    "kernel2 = [[-1, 2, -1],\n",
    "           [-1, 2, -1],\n",
    "           [-1, 2, -1]]\n",
    "\n",
    "kernel3 = [[-1, -1, -1],\n",
    "           [-1, 1, 1],\n",
    "           [-1, 1, 1]]\n",
    "\n",
    "kernel4 = [[1, 1, 1],\n",
    "           [1, 1, 1],\n",
    "           [1, 1, 1]]\n",
    "\n",
    "# Extensionüòà: This is a bit tougher as it's now 3D!\n",
    "kernel5 = [\n",
    "            [\n",
    "                [-1, -1, -1],\n",
    "                [-1, -1, -1],\n",
    "                [2, 2, 2]\n",
    "            ],\n",
    "            [\n",
    "                [-1, -1, -1],\n",
    "                [2, 2, 2],\n",
    "                [-1, -1, -1]\n",
    "            ],\n",
    "            [\n",
    "                [2, 2, 2],\n",
    "                [-1, -1, -1],\n",
    "                [-1, -1, -1]\n",
    "            ]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a 2-dimensional convolutional layer in Pytorch using `torch.nn.conv2d()`. The '2-dimensional' means we are going to run a double for loop over the input - but we can still run this on an RGB image, try to imagine how!\n",
    "\n",
    "Conv2d takes several parameters:\n",
    "- `in_channels`: the number of dimensions we are going to double for loop over. E.g. gor a grayscale this would be 1, but for an RGB images this would be 3.\n",
    "- `out_channels`: A convolutional layer actually consists of many kernels, each responsible for something different e.g. vertical edges, horizontal edges, bright patches, dark patches etc. The number of output channels is the number of kernels in the layer and hence the number of different features we want to learn.\n",
    "- `kernel_size`: A kernel is $n\\times n$ and here is where we choose $n$. (technically $n\\times n \\times \\textnormal{input\\_channels}$: **Thinkü§î**: Why?)\n",
    "\n",
    "There are 2 other parameters of note: `stride` and `padding`. Perhaps you can guess what they do by the name, but you'll figure out what they do in one of the coming exercises. There are other parameters, but they're less important. I'll leave you to read about them [in the docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) if you like.\n",
    "\n",
    "First let's see a basic conv layer in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import torch as well as the nn module\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# A convoloutional layer with a single input channel and a single output channel, with kernel size 2\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2)\n",
    "\n",
    "# We can print the kernel\n",
    "print(conv_layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the kernel to detect vertical edges. Ignore the extra dimensions, they're just for batching images and because the out_channels is 1\n",
    "conv_layer.weight.data = torch.tensor([[\n",
    "            [[-1, 1],\n",
    "           [-1, 1]]\n",
    "           ]]).float()\n",
    "\n",
    "# Let's set the bias to 0 so that it doesn't affect the output\n",
    "conv_layer.bias.data = torch.tensor([0.0])\n",
    "\n",
    "# Here's a 10x10 image with a vertical edge\n",
    "image = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "                          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]).float()\n",
    "\n",
    "# Let's run the image through the conv layer. We need to add extra dimensions to the image for similar reasons as before\n",
    "output = conv_layer(image.view(1, 1, 10, 10))\n",
    "print('Output image: \\n', output)\n",
    "print('='*50)\n",
    "# We can also print out the 'shape' of the output i.e. the size of each dimension \n",
    "print('Output image shape: \\n', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: Can you understand the output? Is it what you expected? Is the dimensionality of the output the same as the input? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now change the padding and see what happens\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, padding=1)\n",
    "conv_layer.weight.data = torch.tensor([[\n",
    "            [[-1, 1],\n",
    "           [-1, 1]]\n",
    "           ]]).float()\n",
    "conv_layer.bias.data = torch.tensor([0.0])\n",
    "output = conv_layer(image.view(1, 1, 10, 10))\n",
    "print('Output image: \\n', output)\n",
    "print('='*50)\n",
    "print('Output image shape: \\n', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: What happened? What's the output/shape now? Why might this be a useful feature of a convolution layer?\n",
    "\n",
    "*Hint: Can you tell what the padding must have done to the input to give you that output? Go have a google if you're struggling.*\n",
    "\n",
    "**Extensionüòà**: There's another parameter you can set here on the `conv_layer` called `padding_mode`. Try setting it to `'reflect'`. What do you think will happen to the output?\n",
    "\n",
    "Let's now look at `stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now change the stride and see what happens\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=2)\n",
    "conv_layer.weight.data = torch.tensor([[\n",
    "            [[-1, 1],\n",
    "           [-1, 1]]\n",
    "           ]]).float()\n",
    "conv_layer.bias.data = torch.tensor([0.0])\n",
    "output = conv_layer(image.view(1, 1, 10, 10))\n",
    "print('Output image: \\n', output)\n",
    "print('='*50)\n",
    "print('Output image shape: \\n', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: Try changing the stride value to 3 then 4 as well. What is happening? Why might this be useful?\n",
    "\n",
    "Let's now run it on a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fetch a real image and display it\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "url = 'https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert('L')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Let's run our edge detection kernel on the image\n",
    "# First we need to convert the image to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# Now we can run the image through the conv layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "\n",
    "# Let's set the kernel to detect vertical edges\n",
    "conv_layer.weight.data = torch.tensor([[\n",
    "            [[-1, 0, 1],\n",
    "           [-1, 0, 1],\n",
    "           [-1, 0, 1]]\n",
    "           ]]).float()\n",
    "conv_layer.bias.data = torch.tensor([0.0])\n",
    "\n",
    "output = conv_layer(img_tensor)\n",
    "# Print the output as a PIL image\n",
    "plt.imshow(output.squeeze().detach().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('='*50)\n",
    "print('Output image shape: \\n', output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: Did it do what you expect? Try messing around with the kernel to see how the image responds! Try changing the stride and other parameters above! \n",
    "\n",
    "When you're happy, we can move onüöÄ\n",
    "\n",
    "## 2. The ConvNet Architecture\n",
    "\n",
    "We've now seen a single convolutional layer, but that's not a full architecture. How is detecting edges useful? Well - imagine you know where all the vertical and horizontal edges are in an image. Maybe you'd have a better guess at where the legs are? Maybe where the nose is? And if you know where the nose and legs are, maybe you can tell where a face and a person is. Or you could tell which kind of animal is in the image. Or maybe you could detect where [cancerous cells in the brain are](https://github.com/adityajn105/brain-tumor-segmentation-unet).\n",
    "\n",
    "But to do that, we're going to need more than one layer. We take the 'image' output from the previous layer, and plug it in as input to the next layer. The deeper you go in the network, the more abstract the concepts become, because it's combining concepts from the previous layer. \n",
    "\n",
    "Remember last week we needed non-linearity of our input so as not to make straight lines? Some similar logic can be applied here, so we apply `ReLU` again. But now we have a much more intuitive reason for using ReLU: Imagine we have run a convolution layer that tries to detect where faces are. Each pixel in the output is a score for how confident we are there is a face at this location. Except, we only really care about the highest scores. Therefore, we can run the ReLU max function to only retain the information about where we are fairly sure there is a face. More theoretically, we represent far more complicated functions with ReLU.\n",
    "\n",
    "We can also improve speed with 'max pooling'. Don't worry too much about this, as it's less important, and it's not the only way to save on computation. This can be run in Pytorch with [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).\n",
    "\n",
    "You can see all this in action at [the homepage for Stanford's course on Computer Vision.](https://cs231n.stanford.edu/)\n",
    "\n",
    "Now you're going to build your own ConvNet!üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple CNN\n",
    "\n",
    "With all the bits in place, lets take a look at how we can define a simple CNN. \n",
    "\n",
    "Usually, the model consists of a base and a head. The **base** is used for **feature extraction** and consists of at least one, but usually more **Convolutional Layers**. The **head** consists of at least on dense layer (tbt the Neural Networks workshop) and takes as input the extracted features from the Convolutional Layer. For example, if we try to detect faces, the convolutional layers in the base would find two circles next to each other (eyes), a vertical and a horizontal line (nose and mouth) and the dense layers in the head would learn to classify this combination of features as a face.\n",
    "\n",
    "Each **Convolutional Layer** in the base is made of of **three basic operations**:\n",
    "1. Convolution **filters** an image for a particular feature\n",
    "2. ReLU **detects** that feature within the filtered image\n",
    "3. Maximum Pooling **condenses** the image (and enhances the features)\n",
    "\n",
    "Let's define our own convolutional layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    ")\n",
    "\n",
    "# Let's see what the output shape is\n",
    "random_image = torch.randn(3, 256, 256) # 3 is the number of channels (RGB), 256 is the height and width of our image\n",
    "output = layer(random_image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a **single Convolutional Layer**, we need to think about how to connect the **head** to it so we can use the extracted features for something! Looking at the size of the output of the convolutional layer, we observer that the image is still in 2D-format. But the dense layer requires a 1D-input. Therefore, we need to apply an operation to transform the data from 2D to 1D. We will use [nn.Flatten()](https://pytorch.org/docs/stable/generated/torch.flatten.html). Lets look at how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D input tensor (3x3)\n",
    "input_2d = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6],\n",
    "                         [7, 8, 9]])\n",
    "\n",
    "print(\"Original 2D input:\")\n",
    "print(input_2d)\n",
    "print(\"Shape:\", input_2d.shape)\n",
    "\n",
    "# Create a Flatten layer\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "# Apply the Flatten operation\n",
    "output_1d = flatten(input_2d.unsqueeze(0))  # We need to do a little trick here, since the layer usually operates on multiple examples of data at the same time (batches).\n",
    "\n",
    "print(\"\\nFlattened 1D output:\")\n",
    "print(output_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tensor is transformed from 3x3 to 1x9. Now we are ready to attach a classification head to our CNN! We will reuse the convolutional layer we used earlier and then add the flatten operation and a linear layer. This is the part you should try to understand best. What is each part doing? What is the dimensionality of the inputs being passed through?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # Base (Find Nose, Mouth, Eyes)\n",
    "\n",
    "    ## Our convolutional layer\n",
    "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1), #¬†Filter\n",
    "    nn.ReLU(), #¬†Detect\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), #¬†Condense\n",
    "\n",
    "    # Head (Classify that combination of features as a face)\n",
    "\n",
    "    ## The flatten layer, which transforms the 2D output of the convolutional layer into a 1D input for the dense layer\n",
    "    nn.Flatten(),\n",
    "\n",
    "    ## The dense layer, which would perform the classification\n",
    "    nn.Linear(32 * 64 * 64, 5)\n",
    ")\n",
    "\n",
    "# Let's see the architecture of our model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try feeding it some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some dummy data to feed into the model\n",
    "batch_size = 3\n",
    "channels = 3\n",
    "height = 128\n",
    "width = 128\n",
    "dummy_input = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# Feed the dummy data through the model\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"\\nModel output shape:\")\n",
    "print(output.shape)\n",
    "\n",
    "print(\"\\nModel output (first sample):\")\n",
    "print(output[0])\n",
    "\n",
    "print('\\n')\n",
    "# Explanation of the output\n",
    "print(f\"The model processes {batch_size} images, each with dimensions {channels}x{height}x{width}.\")\n",
    "print(f\"It outputs {batch_size} sets of 5 numbers, corresponding to the 5 output classes.\")\n",
    "print(\"Each number represents the model's 'confidence' for that class.\")\n",
    "print(\"The highest number indicates the predicted class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the basic structure of a CNN, but currently it only outputs gibberish because we haven't trained it yet. We will first review what it takes to train a NN and then you will create your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recap how we train neural networks. If you need a better refresher, you can look at the notebook from the last workshop.\n",
    "\n",
    "#### Recap: Steps to Train a NN in Pytorch\n",
    "\n",
    "Create \n",
    "- a neural network\n",
    "- a loss function\n",
    "- an optimiser, here you choose the *learning rate*\n",
    "\n",
    "Train the model:\n",
    "- Reset the optimiser with `optimiser.zero_grad()`\n",
    "- Run some data through the model. \n",
    "- Calculate the loss\n",
    "- Backpropagate and calculate gradients with `loss.backward()`\n",
    "- Update model parameters with `optimiser.step()`\n",
    "- Repeat\n",
    "\n",
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your own Classifier\n",
    "\n",
    "Now that we've seen how convolutional layers work and why they're useful, it's time to step things up and train our own classifier! This is where things get exciting: we‚Äôre going to take a dataset of satellite images and train a CNN to classify them into different categories. Satellite image classification is used in a ton of real-world applications. For example, in urban planning, satellite images help track how cities are expanding or where new infrastructure needs to be developed. In environmental monitoring, satellite image classification can help track deforestation üå≥, monitor crop health üåæ, or even assess the damage after natural disasters.\n",
    "\n",
    "We are focusing on a specific challenge: classifying satellite images üåç into different land-use categories. We'll be using the UC Merced Land Use Dataset, which contains 2,100 images across 21 classes, ranging from agricultural areas to tennis courts. This section is pretty much plug-and-play. You won't need to write a lot of code, but feel free to play around and change things.\n",
    "\n",
    "Let's start with importing the images! First, You add the dataset to this notebook. For this:\n",
    "1. Click \"Add Input\"\n",
    "2. Select \"Datasets\" as filter\n",
    "3. Search for \"UC Merced Land Use Dataset\" and add it.\n",
    "\n",
    "**There will be a lot of code in this section that you don't need to worry about!** You will be working on the most relevant bits that help you understand how to train a CNN. Of course, if you are curious about anything else in the code, feel free to ask us!\n",
    "\n",
    "Now let's import the data so we can take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the classes in the dataset\n",
    "CLASSES = ['agricultural', 'airplane', 'baseballdiamond', 'beach', 'buildings', 'chaparral', 'denseresidential', 'forest', 'freeway', 'golfcourse', 'harbor', 'intersection', 'mediumresidential', 'mobilehomepark', 'overpass', 'parkinglot', 'river', 'runway', 'sparseresidential', 'storagetanks', 'tenniscourt']\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = '/kaggle/input/uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Resize((256, 256))  # Resize images to 128x128\n",
    "])\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Testing images: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples to see what we are working with. This code randomly picks 4 images and displays them with their label. Don't worry about how exactly this works, just run it a few times to get a feel for what the data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    img = img.permute(1, 2, 0)  # rearrange dimensions for plotting\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Get 4 random indices\n",
    "random_indices = random.sample(range(len(train_dataset)), 4)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Get the image and label\n",
    "    img, label = train_dataset[idx]\n",
    "    \n",
    "    # Get the class name\n",
    "    class_name = CLASSES[label]\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.sca(axs[i])\n",
    "    imshow(img, title=class_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: What do you think the challenges are for training a classifier on this dataset? Are there any classes that might be harder to distinguish than others?\n",
    "\n",
    "Let's start by setting up a DataLoader, which helps us feed the images to the classifier in a convenient way. We define one each for our train and test set. This is just boilerplate infrastructure code we need to get our model off the ground, so don't worry about it too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
    "\n",
    "# Verify the shape of a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Shape of image batch: {images.shape} [number of images in the batch, RGB, height, width]\")\n",
    "    print(f\"Shape of label batch: {labels.shape} [number of images in the batch]\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our Convolutional Neural Network. You can copy the convolutional layer we defined earlier. Usually, we define multiple convolutional layers in the base to better extract features. Add another one or two! Then, add the flatten operation and one or more fully connected Layers. The output of the final Linear Layer needs to correspond to the number of output classes in our dataset (21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = nn.Sequential(\n",
    "    # Convolutional layer 1\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    # Flatten the output for the fully connected layers\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    nn.Linear(128 * 32 * 32, 512),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(512, 21)  # 21 output classes as per the UC Merced Land Use Dataset\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the CNN! **Make sure to activate GPU-acceleration to speed up the training**! You can do this by \n",
    "1. Clicking \"settings\" at the top\n",
    "2. Going to \"Accelerator\"\n",
    "3. Clicking GPU P100\n",
    "4. If you are prompted whether you want to turn it on, just click \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how well our model does on unseen data (our test-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think our model is performing well on unseen data? \n",
    "\n",
    "One good way of visualising the model's classification performance is looking at a Confusion Matrix. It's called a \"confusion\" matrix because it makes it easy to see if the model is confusing two classes (i.e., mislabeling one class as another). A confusion matrix shows the true labels on one axis and the model's predictions on the other, making it easy to see where the model is accurate and where it's making mistakes. The diagonal of the matrix shows correct predictions, while off-diagonal elements reveal misclassifications, allowing you to quickly identify which classes the model struggles to distinguish.\n",
    "\n",
    "Let's generate one for the model you have trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "class_names = CLASSES\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Uncomment the following lines to see a more detailed report of model performance\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(all_labels, all_predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows what the true labels are on the y-axis and what our model predicted. \n",
    "As you can see, the model does not perform very well. \n",
    "\n",
    "**Thinkü§î**: Can you spot any patterns where the model does well and where it has a hard time distinguishing between classes?\n",
    "\n",
    "I think we can do a lot better than this! There are several ways we could try to improve the performance. One is to try to define a better model architecture. Since the current model is fairly simple, we would probably need to define something more complex. But then training would take even longer! \n",
    "\n",
    "Another option is to use an already trained CNN that is very good at feature detection and just add our own classification head. Lets try to do that instead! We will use [Inception-v3](https://pytorch.org/hub/pytorch_vision_inception_v3/). This is a great option because some people spent a lot of time and money on training it and we can just build on top of that! Just run the cell below and it will take care of training & testing.\n",
    "\n",
    "Dont worry about the details of the code too much - essentially we are doing the exact same thing as before, except for that our base layer now uses Inception-v3 instead of our own convolutional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained Inception v3 model\n",
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "num_classes = len(CLASSES)\n",
    "\n",
    "# Add a classification head consisting of 4 layers to the pre-trained model.\n",
    "model.fc = nn.Sequential(\n",
    "    # Base\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "\n",
    "    # Classification Head\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes)\n",
    "    )\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Inception v3 expects input size of 299x299\n",
    "        images = nn.functional.interpolate(images, size=(299, 299))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Inception v3 returns tuple in training mode\n",
    "        outputs, aux_outputs = model(images)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4*loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this model's performance on the unseen test-data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = nn.functional.interpolate(images, size=(299, 299))\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, look at the confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = nn.functional.interpolate(images, size=(299, 299))\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "class_names = CLASSES\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Uncomment the following lines to see a more detailed report of model performance\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(all_labels, all_predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, this version performs A LOT better than our previous attempt! Making use of pre-trained models can be very helpful, and a lot of models are available for free directly within [PyTorch](https://pytorch.org/hub/research-models). You can also check out [HuggingFace](www.huggingface.com), where people share a lot of datasets and pre-trained models that you can use. This comes in very handy if you are working on a project of your own and don't want to bother dealing with low-level details of the CNN-architecture.\n",
    "\n",
    "**Thinkü§î:** Still, the model has some trouble distinguishing between some classes. Do you have an idea of how we could improve on that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segment your face!\n",
    "\n",
    "Let's now use a more advanced model that's already been trained (pre-trained) to segment your own face! The two cells below current fetches an image from online. Try running the cells below.\n",
    "\n",
    "To use your own custom image (e.g. you), you'll need to upload it to Kaggle and load it in. To do that, under 'Input', click Upload, and upload an image you want to segment. Then name the dataset something like 'my-image', then edit the code in the cell below to load it in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# TODO: Comment the code below to load in your own custom image\n",
    "# This cell loads an image and displays it\n",
    "url = \"https://images.unsplash.com/photo-1539571696357-5a69c17a67c6\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# TODO: Uncomment the line below, and edit it to load in your own custom image\n",
    "# image = Image.open('../input/NAME_OF_DATASET/NAME_OF_FILE_WITH_EXTENSION')\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell segments the image using a pretrained model and displays the segmentation. Don't edit the code!\n",
    "\n",
    "# convenience expression for automatically determining device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    # Device for NVIDIA or AMD GPUs\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    # Device for Apple Silicon (Metal Performance Shaders)\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# load models\n",
    "image_processor = SegformerImageProcessor.from_pretrained(\"jonathandinu/face-parsing\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"jonathandinu/face-parsing\")\n",
    "model.to(device)\n",
    "\n",
    "# run inference on image\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, ~height/4, ~width/4)\n",
    "\n",
    "# resize output to match input image dimensions\n",
    "upsampled_logits = nn.functional.interpolate(logits,\n",
    "                size=image.size[::-1], # H x W\n",
    "                mode='bilinear',\n",
    "                align_corners=False)\n",
    "\n",
    "# get label masks\n",
    "labels = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "# move to CPU to visualize in matplotlib\n",
    "labels_viz = labels.cpu().numpy()\n",
    "# Show both images with opacity 0.5\n",
    "plt.imshow(image)\n",
    "plt.imshow(labels_viz, cmap='tab20', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision ‚úÖ\n",
    "\n",
    "> That's it! I hope you can see how cool Convolutions are, and how powerful they are. They're the backbone of lots of what makes machine learning so powerful nowadays. Go and building something cool with them!\n",
    "> If you want to learn more, go check out [this course at Stanford](https://cs231n.github.io/convolutional-networks/), which has loads more details.\n",
    ">\n",
    "> See you next week for some language models and ChatGPT!\n",
    "\n",
    "Pierre Mackenzie & Valentin Magis, Edinburgh AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edinburgh-ai-workshops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
