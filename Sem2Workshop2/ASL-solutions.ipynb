{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2: Using Computer Vision for Good! ğŸš€\n",
    "\n",
    "- **When:** Wednesday, February 26th, 17:00.\n",
    "- **Where:** Appleton Tower, 5.04. If you don't have access, reach out and we'll let you in!\n",
    "- **Contacts:** Reach out on Instagram _@edinburgh.ai_\n",
    "- **Credits:** This notebook is created by EdinburghAI for use in its workshops. If you plan to use it, please credit us!\n",
    "\n",
    "## Today\n",
    "- Today we're building a Sign Language Interpreter. \n",
    "- We'll take hundreds of photos of different hand signs and extract the relevant information.\n",
    "- We'll try training different models.\n",
    "- We'll then upload our own hand sign and see if our model accurately predicts it!\n",
    "\n",
    "Lfg ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "## IMPORTANT! Turn On Internet\n",
    "1. On the right-hand side of this notebook, there's a section called **\"Session Options\"**.\n",
    "2. Scroll down to the _\"Internet\"_ toggle. Turn it on. You may need to verify your phone number.\n",
    "3. Additionally, to help this run faster, you can also enable some GPU access.\n",
    "\n",
    "\n",
    "## Using Jupyter:\n",
    "This is a Jupyter notebook. It contains cells. There are 2 kinds of cells - markdown and Python. Markdown cells are like this one, and are just there to give you information. Python cells run code. You can run a cell with `Ctrl + Enter` or run a cell and move the next one with `Shift + Enter`. Try running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ctrl + Enter runs this cell!')\n",
    "output = 'The last line of a cell is printed by default'\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install mediapipe\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Our Data\n",
    "\n",
    "Like all good AI projects, we need to explore our data first. Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, let's take a look at the data. \n",
    "\n",
    "# We'll be using OpenCV for this project.\n",
    "# OpenCV is a popular library for computer vision.\n",
    "import cv2\n",
    "\n",
    "# Let's load in an image and display it.\n",
    "img = cv2.imread('/kaggle/input/synthetic-asl-alphabet/Train_Alphabet/A/0042513a-63c0-499f-a7f7-e6ee1266cb98.rgb_0000.png')\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm. That doesn't look quite right. Turns out OpenCV defaults to BlueGreenRed, but our eyes do not lol. \n",
    "\n",
    "ğŸ¤” _Why do you think that is?_ \n",
    "\n",
    "Regardless, let's turn it to RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Ok sweet. We're gonna do a few things from here. \n",
    "- We're gonna take all of the crucial parts of the hand (wrist, phalanges lol, etc.) and then feed _that_ into a model. \n",
    "- We're gonna (inefficiently) feed the entire image into a neural network and pray. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Key Features of the Hand\n",
    "\n",
    "#### Ok... How?\n",
    "We're gonna use a library called _MediaPipe_. Some really smart people made a suite of models - we're going to use the _Hands_ model. This will try and identify if there are any hands in the photo. If there are, we'll place coordinates on top of the photo. \n",
    "\n",
    "Let's try it out!\n",
    "\n",
    "### Quickly, what will it look like?\n",
    "Let's use the MediaPipe Drawing Utils to try and draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "hand_landmarks = hands.process(img_rgb)\n",
    "\n",
    "for landmark in hand_landmarks.multi_hand_landmarks:\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "            img_rgb,  # image to draw\n",
    "            landmark,  # model output\n",
    "            mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "    plt.imshow(img_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great. Now get those landmarks for every image.\n",
    "\n",
    "Sweet. So we found out that `mp.solutions.hands` finds an image and turns it the landmarks of a hand into a bunch of coordinates. \n",
    "\n",
    "_Also as a side note, we're going to normalise them so all of the coordinates are relative to 0 (as opposed to where they actually were in the photo)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/synthetic-asl-alphabet/Train_Alphabet\"\n",
    "\n",
    "## MAKE DATA FROM NORMAL IMAGES\n",
    "def turnImagesToLandmarks(DATA_DIR):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    \n",
    "    hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for dir_ in (os.listdir(DATA_DIR)):\n",
    "        for img_path in tqdm(os.listdir(os.path.join(DATA_DIR, dir_))[:30]):\n",
    "            data_aux = []\n",
    "    \n",
    "            x_ = []\n",
    "            y_ = []\n",
    "    \n",
    "            img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            results = hands.process(img_rgb)\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                hand_landmarks = results.multi_hand_landmarks[0]  # Only take the first hand\n",
    "                x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "                y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "                data_aux = []\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    data_aux.append(x_[i] - min(x_))\n",
    "                    data_aux.append(y_[i] - min(y_))\n",
    "                data.append(data_aux)\n",
    "                labels.append(dir_)\n",
    "\n",
    "        print(f\"Finished letter {dir_}\")\n",
    "    return labels, data\n",
    "    \n",
    "\n",
    "\n",
    "labels, data = turnImagesToLandmarks(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ummm. Did that work? Let's take a look at the what we made..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T20:20:32.918724Z",
     "iopub.status.busy": "2025-02-24T20:20:32.918341Z",
     "iopub.status.idle": "2025-02-24T20:21:16.402649Z",
     "shell.execute_reply": "2025-02-24T20:21:16.401442Z",
     "shell.execute_reply.started": "2025-02-24T20:20:32.918696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.5)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2->mediapipe) (2.4.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->mediapipe) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2->mediapipe) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2->mediapipe) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2->mediapipe) (2024.2.0)\n",
      "Downloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl (35.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: protobuf, sounddevice, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.6 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mediapipe-0.10.21 protobuf-4.25.6 sounddevice-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# Let's print out only the first data point and first label.\n",
    "data[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweet! That seems right...\n",
    "Now let's train a model. Let's use RandomForest. Don't forget to split the data into a corresponding train-test split and then get the accuracy. \n",
    "\n",
    "If all of that seems like non-sense, take a look at our first workshop we went [through in Sem1](https://github.com/EdinburghAI/workshops/blob/main/Sem1Workshop1/IntroToML/IntroToML-Solved.ipynb) - Scroll down to _\"Decision Trees\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle  # For model saving & loading\n",
    "\n",
    "# âœ… Load your trained data (assuming `data` & `labels` are already prepared)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "# âœ… Train the Random Forest Model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# âœ… Evaluate Model\n",
    "y_predict = model.predict(x_test)\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "print('{}% of samples were classified correctly!'.format(score * 100))\n",
    "\n",
    "# âœ… Save Model for Later Use\n",
    "with open('asl_hand_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That was fast! \n",
    "The model trained reallyyyy quickly. In ML, you often don't have to pick the fanciest model. \n",
    "\n",
    "We're basically taking coordinates and trying to find a shape between them - therefore our model doesn't need to be the most complicated. This makes everything wayyy faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Time\n",
    "Now let's test it out! Take a photo on your phone of you doing a Sign and see if the model will recognise it!\n",
    "\n",
    "\n",
    "To do this:\n",
    "- Take a photo on your phone. Ensure there's only 1 hand visible in the frame.\n",
    "- Pass the photo to your laptop.\n",
    "- On the right hand side, scroll to the _\"Upload\"_ button > _\"New Dataset\"_. \n",
    "- Give it a name\n",
    "- Press create!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# ğŸš€ Inference Function\n",
    "# -------------------------\n",
    "def infer_single_image(img_path, model_path='asl_hand_model.pkl'):\n",
    "    \"\"\"\n",
    "    Takes an image path, extracts hand landmarks, and predicts the ASL letter using the trained model.\n",
    "    \"\"\"\n",
    "    # âœ… Load the trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # âœ… Initialize Mediapipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "    # âœ… Load and preprocess image\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]  # Only take the first detected hand\n",
    "        x_ = np.array([lm.x for lm in hand_landmarks.landmark])\n",
    "        y_ = np.array([lm.y for lm in hand_landmarks.landmark])\n",
    "\n",
    "        # âœ… Normalize features (same as training)\n",
    "        x_min, y_min = x_.min(), y_.min()\n",
    "        data_aux = np.column_stack((x_ - x_min, y_ - y_min)).flatten().tolist()\n",
    "\n",
    "        # âœ… Predict using trained model\n",
    "        prediction = model.predict([data_aux])[0]\n",
    "\n",
    "        return prediction  # Return predicted label\n",
    "    else:\n",
    "        return \"No hand detected!\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ğŸš€ Example Usage\n",
    "# -------------------------\n",
    "img_path = \"/kaggle/input/randomphotoofa/IMG_4192.JPG\"  # Replace with your actual test image\n",
    "predicted_label = infer_single_image(img_path)\n",
    "print(f\"Predicted ASL Letter: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 23079,
     "sourceId": 29550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2275641,
     "sourceId": 3821116,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
