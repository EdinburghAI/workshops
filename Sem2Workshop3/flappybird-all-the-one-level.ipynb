{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Workshop: Reinforcement Learning with Flappy Bird üê¶\n",
    "\n",
    "- **When:** 12 March, 17:00\n",
    "- **Where:** Appleton Tower, 5.04\n",
    "- **Contacts:** Reach out on Instagram *@edinburgh.ai*\n",
    "- **Credits:** Created by EdinburghAI for educational purposes. If you use it, please credit us!\n",
    "\n",
    "## Today‚Äôs Mission üöÄ\n",
    "\n",
    "Today, we'll teach an AI to master **Flappy Bird** using Reinforcement Learning (RL). We'll:\n",
    "- Set up the Flappy Bird environment\n",
    "- Explore how RL agents perceive their environment and make decisions\n",
    "- Understand key RL concepts like state, action, reward, and training loops\n",
    "- Train our bird to fly autonomously!\n",
    "\n",
    "Lfg! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "## IMPORTANT! Turn On Internet\n",
    "1. On the right-hand side of this notebook, there's a section called **\"Session Options\"**.\n",
    "2. Scroll down to the _\"Internet\"_ toggle. Turn it on. You may need to verify your phone number.\n",
    "3. Additionally, to help this run faster, you can also enable some GPU access.\n",
    "\n",
    "\n",
    "## Using Jupyter:\n",
    "This is a Jupyter notebook. It contains cells. There are 2 kinds of cells - markdown and Python. Markdown cells are like this one, and are just there to give you information. Python cells run code. You can run a cell with `Ctrl + Enter` or run a cell and move the next one with `Shift + Enter`. Try running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ctrl + Enter runs this cell!')\n",
    "output = 'The last line of a cell is printed by default'\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß© Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "In Reinforcement Learning (RL), an agent learns by interacting continuously with an environment. At each step, the agent goes through the following cycle:\n",
    "\n",
    "- **Agent**: Our bird! In RL, it's normally the AI we're training.\n",
    "- **Observation (State)**: This is everything that the agent sees. How to setup the environment is not necessarily obvious. For example:\n",
    "  - We could just feed in the raw pixels of the game.\n",
    "  - We could feed in the relative locations of the pipes.\n",
    "  - Or we could feed in the a Lidar scan of everything the bird \"sees\".\n",
    "  - Our agent will be able to see just the relative locations of the upcoming pipes.\n",
    "- **Action (Space)**: These are the set of all possible actions one can take. For our bird, it's quite simple; flap or don't flap. But what about an agent learning to play go? It would be MUCH larger.\n",
    "- **Reward**: After performing an action, the agent receives a numerical reward. This reward signals whether the action was good or bad, guiding the agent to improve its behavior over time. The rewards for our bird is:\n",
    "  - +0.1 - every frame it stays alive\n",
    "  - +1.0 - successfully passing a pipe\n",
    "  - -1.0 - dying\n",
    "  - -0.5 - touch the top of the screen\n",
    "- **Next Observation**: The action then gets fed back into the environment, the environment changes, we make an obvservation and repeat the cycle again.\n",
    "\n",
    "Here's how the cycle looks visually:\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/AE_loop.png\" alt=\"Reinforcement Learning Cycle\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Our goal is to teach our agent (the Flappy Bird) to navigate through pipes successfully, maximizing its cumulative reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started! üß®\n",
    "We're going to have to install some dependencies first. Run the below cell to get everything we need.\n",
    "\n",
    "What we're importing:\n",
    "\n",
    "- **gymnasium & flappy_bird_gymnasium:** Provides the Flappy Bird game environment\n",
    "- **torch (PyTorch):** Deep learning library for building neural networks\n",
    "- **collections.deque:** Efficient data structure for our experience replay memory\n",
    "- **matplotlib & IPython.display:** For visualizing our agent's performance\n",
    "- **tqdm:** For progress bars during training and downloading\n",
    "\n",
    "Our code checks if a GPU is available and uses it if possible. GPUs can be 10-100x faster than CPUs for neural network training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flappy-bird-gymnasium\n",
    "!pip install gymnasium\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from datetime import datetime\n",
    "import time\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the environment\n",
    "Let's dive a little more into the actual environment that the model runs in. We spoke about it above briefly. Try fill in the below. Details on this specific environment are [here](https://github.com/markub3327/flappy-bird-gymnasium), and details about interacting with environments are [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0', render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Let's look at what information we get from the environment\n",
    "print(\"1. What does the observation space look like?\")\n",
    "# TODO use some print statements to explore the observation space!\n",
    "# Try looking at things like:\n",
    "# -- the actual observation space\n",
    "# -- the shape of the space\n",
    "# -- how many dimensions (numbers) that represent the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "180! that is a lot of data, This observation space uses the lidar scan infront of the bird and thus there is a lot of extra data.\n",
    "Could we potentially compress this information to use only the most important information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "\n",
    "# What about when we set lidar to false?\n",
    "env = gym.make('FlappyBird-v0', render_mode=\"rgb_array\", use_lidar=False)\n",
    "\n",
    "# Let's look at what information we get from the environment\n",
    "# TODO do the same again and compare the results,\n",
    "\n",
    "# This is significantly less information, \n",
    "# can you think of a way we might be capturing only the useful information from the enviroment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers represent? Well, according to their documentation, they are:\n",
    "\n",
    "- the last pipe's horizontal position\n",
    "- the last top pipe's vertical position\n",
    "- the last bottom pipe's vertical position\n",
    "- the next pipe's horizontal position\n",
    "- the next top pipe's vertical position\n",
    "- the next bottom pipe's vertical position\n",
    "- the next next pipe's horizontal position\n",
    "- the next next top pipe's vertical position\n",
    "- the next next bottom pipe's vertical position\n",
    "- player's vertical position\n",
    "- player's vertical velocity\n",
    "- player's rotation\n",
    "\n",
    "Now lets explore the action space!\n",
    "This is the space of available actions our model can perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2. What actions can our agent take?\")\n",
    "\n",
    "# TODO Similar to what we did above can you explore:\n",
    "# -- the action space\n",
    "# -- the number of possible actions\n",
    "# think about what these actions may represent\n",
    "\n",
    "print(\"In Flappy Bird, action 0 = do nothing, action 1 = flap wings\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state, info = env.reset()\n",
    "print(\"3. What does the initial state look like?\")\n",
    "\n",
    "# now lets explore what the intial state looks like, \n",
    "# TODO add a print statement to show the intial state\n",
    "\n",
    "print(f\"\"\"The state represents: \n",
    "      \n",
    "      - the last pipe's horizontal position\n",
    "      - the last top pipe's vertical position\n",
    "      - the last bottom pipe's vertical position\n",
    "      - the next pipe's horizontal position\n",
    "      - the next top pipe's vertical position\n",
    "      - the next bottom pipe's vertical position\n",
    "      - the next next pipe's horizontal position\n",
    "      - the next next top pipe's vertical position\n",
    "      - the next next bottom pipe's vertical position\n",
    "      - player's vertical position\n",
    "      - player's vertical velocity\n",
    "      - player's rotation\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, terminated, truncated, info = env.step(1)  # Take action \"flap\"\n",
    "print(\"\\n4. After taking action 'flap':\")\n",
    "# Now we have taken a step we can asses the next state\n",
    "# TODO try printing some of the aspects of our enviroment\n",
    "# try:\n",
    "# -- next state\n",
    "# -- reward\n",
    "# -- the terminated flag\n",
    "\n",
    "# We can also render the environment to see what's happening\n",
    "frame = env.render()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Flappy Bird Environment\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î What is a Q-Network?\n",
    "A Q-Network is a neural network that estimates \"Q-values\" - the expected future rewards for taking each possible action in a given state. In Flappy Bird:\n",
    "\n",
    "- **Input (state_dim):** Features describing the game state (bird's position, pipe positions)\n",
    "- **Output (action_dim):** Q-values for each possible action (flap or don't flap)\n",
    "- **Hidden Layer:** Learns patterns and relationships between states and optimal actions\n",
    "\n",
    "**Note:** Today is about the RL environment and setup, so we don't go too in depth about the network (if you want an overview on them, go to our Sem1 Workshop 1!). For now, just think about it like a black box that takes in inputs and outputs the action it thinks is best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        # Initialize the network\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the layers. Note, our model just has a single hidden layer.\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        #¬†Define the output layer. It's the amount of possible outputs (2 for Flappy Bird)\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    # Just a simple forward pass of the network. \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        Q = self.output(x)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok so I kinda lied.\n",
    "\n",
    "When I spoke about how we trained it, I implied it works in a loop. It technically does, but there's a bit more to it. In practice, we use a technique called **Experience Replay** to make training more efficient and stable.\n",
    "\n",
    "Instead of learning from each experience immediately, our agent:\n",
    "1. Collects experiences (state, action, reward, next state) in a \"memory\" buffer\n",
    "2. When it's time to learn, it randomly samples examples from this buffer.\n",
    "3. Updates its neural network based on these experiences\n",
    "\n",
    "\n",
    "The `ReplayMemory` class we defined earlier handles this functionality, storing transitions and allowing us to sample from them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "        \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Understanding Key Hyperparameters:\n",
    "\n",
    "When training a model, there's dials and knobs we can turn to influence the learning process. First, let's explain the exploitation vs exploroation dilemma in RL. \n",
    "\n",
    "### Weirdly contrived scenario üë∂:\n",
    "Imagine a weirdly dexterous baby was plopped into a room with a table and some jenga blocks. It's literally never seen jenga blocks before. \n",
    "\n",
    "We give the baby sugar when it stacks blocks higher than it has previously. (I.e. The reward function is the height of the tower).\n",
    "\n",
    "First, it kinda has to learn how to stack blocks. It's gonna try random actions, drop the blocks, and see what happens. But it really quickly discovers that if it stacks it sideways, it gets more sugar, quickly. In it's _exploration phase_, it learned to stack sideways. \n",
    "\n",
    "It's able to stack them pretty quickly, pretty high. It learns, _\"Hey. This is a tried and tested method. Stack them on their side, get sugar\"_. This is the \"exploitation phase\".\n",
    "\n",
    "But after around 3 sideways blocks, the tower becomes too unstable and just falls. But because it refuses to try any other method, it's stuck on the 3 block tower. \n",
    "\n",
    "If it explored more, it likely would have learned to stack them flat, or in a pyramid. But maybe that would've taken too long to learn. \n",
    "\n",
    "\n",
    "### How that contrived example relates to RL:\n",
    "Our agent starts by trying random actions. It's exploring. The percentage of the time it does something random is called the _exploration rate_. We start with a high exploration rate, and then decrease it as it learns more. There's also a minimum exploration rate, so it never stops exploring completely. \n",
    "\n",
    "\n",
    "**Exploration Parameters (Epsilon):**\n",
    "\n",
    "- `epsilon_init`: We start fully exploratory (100% random actions)\n",
    "- `epsilon_decay`: After each learning step, we reduce exploration by this factor\n",
    "- `epsilon_min`: We'll always maintain some exploration (5%)\n",
    "\n",
    "\n",
    "**Learning Parameters:**\n",
    "\n",
    "- `learning_rate`: How quickly the neural network updates its weights\n",
    "- `discount_factor`: How much we value future rewards compared to immediate ones\n",
    "- `mini_batch_size`: How many experiences we learn from in each update\n",
    "\n",
    "\n",
    "**Memory Parameters:**\n",
    "\n",
    "- `replay_memory_size`: How many past experiences we store (100,000)\n",
    "\n",
    "\n",
    "**Network Parameters:**\n",
    "\n",
    "- `network_sync_rate`: How often we update the target network\n",
    "- `fc1_nodes`: Size of our neural network's hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # Which environment are we using?\n",
    "    \"env_id\": \"FlappyBird-v0\",\n",
    "    \n",
    "    # Memory size for storing past experiences\n",
    "    \"replay_memory_size\": 100000,\n",
    "    \n",
    "    # Number of experiences to sample for each learning update\n",
    "    \"mini_batch_size\": 32,\n",
    "    \n",
    "    # Initial exploration rate (1.0 = 100% random actions)\n",
    "    \"epsilon_init\": 1.0,\n",
    "    \n",
    "    # Rate at which exploration decreases\n",
    "    \"epsilon_decay\": 0.9995,\n",
    "    \n",
    "    # Minimum exploration rate (never explore less than this)\n",
    "    \"epsilon_min\": 0.05,\n",
    "    \n",
    "    # How often to update the target network (in steps)\n",
    "    \"network_sync_rate\": 10,\n",
    "    \n",
    "    # Learning rate for the neural network\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \n",
    "    # Discount factor for future rewards\n",
    "    \"discount_factor\": 0.99,\n",
    "    \n",
    "    # Training will stop if agent gets this much reward in one episode\n",
    "    \"max_reward_threshold\": 10000,\n",
    "    \n",
    "    # Neural network hidden layer size\n",
    "    \"fc1_nodes\": 256,\n",
    "    \n",
    "    # Parameters about the environment we're going to use.\n",
    "    #¬†In our case, we just want the locations of the pipes (relative to us), \n",
    "    #¬†and not the lidar scanning of the bird. \n",
    "    \"env_make_params\": {\"use_lidar\": False},\n",
    "    \n",
    "    # Stop training on reward threshold\n",
    "    \"stop_on_reward\": 10000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Creating the Agent\n",
    "\n",
    "Below is our Agent class that coordinates everything:\n",
    "\n",
    "- Environment setup\n",
    "- Decision making (choosing actions)\n",
    "- Learning from experiences (training)\n",
    "- Managing trained models\n",
    "\n",
    "You don't need to fully understand the code, it's beyond scope of today's workshop. You should be able to run it and be fine. \n",
    "\n",
    "Below it, we're going to work on the training loop, specifically how the learning rate drops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Set hyperparameters\n",
    "        self.env_id = hyperparameters['env_id']\n",
    "        self.learning_rate_a = hyperparameters['learning_rate']\n",
    "        self.discount_factor_g = hyperparameters['discount_factor']\n",
    "        self.network_sync_rate = hyperparameters['network_sync_rate']\n",
    "        self.replay_memory_size = hyperparameters['replay_memory_size']\n",
    "        self.mini_batch_size = hyperparameters['mini_batch_size']\n",
    "        self.epsilon_init = hyperparameters['epsilon_init']\n",
    "        self.epsilon_decay = hyperparameters['epsilon_decay']\n",
    "        self.epsilon_min = hyperparameters['epsilon_min']\n",
    "        self.stop_on_reward = hyperparameters['stop_on_reward']\n",
    "        self.fc1_nodes = hyperparameters['fc1_nodes']\n",
    "        self.env_make_params = hyperparameters.get('env_make_params', {})\n",
    "        \n",
    "        # Neural Network loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # For visualisation in notebook\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        \n",
    "    def create_environment(self, render_mode=\"rgb_array\"):\n",
    "        \"\"\"Create environment with specified render mode\"\"\"\n",
    "        return gym.make(self.env_id, render_mode=render_mode, **self.env_make_params)\n",
    "        \n",
    "    #¬†Don't worry too much about this. \n",
    "    # This is just how we tell the model it's done something wrong \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        # Transpose and separate elements\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
    "        \n",
    "        # Stack tensors to create batch tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        new_states = torch.stack(new_states)\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Calculate target Q values\n",
    "            target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
    "        \n",
    "        # Calculate Q values from current policy\n",
    "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        \n",
    "        # Optimize the model (backpropagation)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "        \n",
    "    def train(self, num_episodes=1000, update_display_every=5):\n",
    "        pass  # We'll be making this in the next cell!\n",
    "    \n",
    "    # Run a trained model and display gameplay in notebook\n",
    "    def run_trained_model(self, model_path=\"best_flappy_model.pt\", max_steps=1000, fps=30):\n",
    "        \"\"\"\n",
    "        Run a trained model and display gameplay in a Jupyter notebook\n",
    "        using matplotlib animation instead of pygame window.\n",
    "        This method is specifically designed for cloud Jupyter notebooks.\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path: Path to the saved model\n",
    "        - max_steps: Maximum number of steps to run\n",
    "        - fps: Frames per second for the display\n",
    "        \n",
    "        Returns:\n",
    "        - Total reward achieved\n",
    "        \"\"\"\n",
    "        # Setup and prepare the visualisation\n",
    "        frames = self.collect_gameplay_frames(model_path, max_steps)\n",
    "        \n",
    "        # Display animation using matplotlib animation\n",
    "        return self.display_gameplay_animation(frames)\n",
    "    \n",
    "    # Separate function to collect gameplay frames\n",
    "    def collect_gameplay_frames(self, model_path=\"best_flappy_model.pt\", max_steps=1000):\n",
    "        \"\"\"Collect frames from the agent playing the game\"\"\"\n",
    "        # Create environment with rgb_array render mode for notebook display\n",
    "        env = self.create_environment(render_mode=\"rgb_array\")\n",
    "        \n",
    "        # Get state and action dimensions\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "        # Create and load the trained model\n",
    "        policy_network = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        policy_network.load_state_dict(torch.load(model_path))\n",
    "        policy_network.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Initialize environment\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Game loop variables\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "        step = 0\n",
    "        \n",
    "        # Print progress message\n",
    "        print(\"Collecting gameplay frames... Please wait.\")\n",
    "        \n",
    "        # Collect frames while playing\n",
    "        while not (terminated or truncated) and step < max_steps:\n",
    "            # Select action based on policy\n",
    "            with torch.no_grad():\n",
    "                action = policy_network(state.unsqueeze(dim=0)).squeeze().argmax().item()\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Convert state to tensor for next iteration\n",
    "            state = torch.tensor(next_state, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Capture frame\n",
    "            frames.append(env.render())\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Show progress periodically\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Collected {step} frames, current reward: {total_reward:.1f}\")\n",
    "        \n",
    "        env.close()\n",
    "        print(f\"Game completed! Total reward: {total_reward:.1f}, collected {len(frames)} frames\")\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    # Display the collected frames as a smooth animation\n",
    "    def display_gameplay_animation(self, frames):\n",
    "        \"\"\"Display collected frames as a smooth animation in the notebook\"\"\"\n",
    "        if not frames:\n",
    "            print(\"No frames to display!\")\n",
    "            return 0\n",
    "            \n",
    "        print(f\"Creating animation with {len(frames)} frames...\")\n",
    "        \n",
    "        # Create figure for the animation\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        plt.close()  # Close the figure to prevent it from displaying twice\n",
    "        \n",
    "        # Initialize with the first frame\n",
    "        img = ax.imshow(frames[0])\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Update function for animation\n",
    "        def update(frame):\n",
    "            img.set_array(frame)\n",
    "            return [img]\n",
    "        \n",
    "        # Create the animation with a faster interval for smoother playback\n",
    "        animation = FuncAnimation(\n",
    "            fig, update, frames=frames, \n",
    "            interval=20,  # Smaller interval for smoother animation\n",
    "            blit=True\n",
    "        )\n",
    "        \n",
    "        # Display the animation in the notebook\n",
    "        return HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the train method\n",
    "def train(self, num_episodes=1000, update_display_every=5):\n",
    "    \"\"\"Train the agent for a specified number of episodes\"\"\"\n",
    "    \n",
    "    ############################################\n",
    "    ######## Setting up the model params #######\n",
    "    ############################################\n",
    "\n",
    "    # Create instance of environment\n",
    "    env = self.create_environment()\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    num_states = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Create neural networks\n",
    "    policy_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "    target_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "    \n",
    "    # Initialise optimizer\n",
    "    self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "    \n",
    "    # Initialise replay memory\n",
    "    memory = ReplayMemory(self.replay_memory_size)\n",
    "    \n",
    "    # Initialise epsilon\n",
    "    epsilon = self.epsilon_init\n",
    "    \n",
    "    # Lists to track progress\n",
    "    rewards_per_episode = []\n",
    "    epsilon_history = []\n",
    "    step_count = 0\n",
    "    \n",
    "    # Initialise best reward\n",
    "    best_reward = -float('inf')\n",
    "    best_model_path = \"best_flappy_model.pt\"\n",
    "    \n",
    "    # Create plot for visualisation\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "\n",
    "    ############################################\n",
    "    ####### Where the model actually learns ####\n",
    "    ############################################\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        #¬†Just some variables for keeping track of things.\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_reward = 0.0\n",
    "        episode_loss = 0.0\n",
    "        num_opt_steps = 0\n",
    "        \n",
    "        # The main loop for the agent. Runs until a single flappy bird run episode is over. (i.e. dies or is so cracked that it runs infinitely - don't worry, it'll be the first one lol)\n",
    "        while not (terminated or truncated) and episode_reward < self.stop_on_reward:\n",
    "            # Select action based on epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                # Select random action\n",
    "                action = env.action_space.sample()\n",
    "                action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "            else:\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "            \n",
    "            # Execute action\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            \n",
    "            # Accumulate rewards\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Convert new state and reward to tensors\n",
    "            new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "            reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Save experience into memory\n",
    "            memory.append((state, action, new_state, reward, terminated))\n",
    "            \n",
    "            # Increment step counter\n",
    "            step_count += 1\n",
    "            \n",
    "            # Train if enough experiences collected\n",
    "            if len(memory) > self.mini_batch_size:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                loss = self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "                episode_loss += loss\n",
    "                num_opt_steps += 1\n",
    "                \n",
    "###################################################################################################\n",
    "                # -- TODO COMPLETE -- \n",
    "    \n",
    "                # Update the epsilon. We want it to be the bigger of the two:\n",
    "                # 1. The current epsilon times the decay rate. \n",
    "                # 2. The minimum epsilon.\n",
    "                # TODO complete our epsilon function!\n",
    "                epsilon = ...\n",
    "###################################################################################################\n",
    "                \n",
    "                # Sync networks\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count = 0\n",
    "            \n",
    "            # Move to next state\n",
    "            state = new_state\n",
    "        \n",
    "        # Calculate average loss for this episode\n",
    "        avg_loss = episode_loss / max(1, num_opt_steps)\n",
    "        \n",
    "        # Store epsilon at the end of episode\n",
    "        epsilon_history.append(epsilon)\n",
    "        \n",
    "        # Track rewards\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "        \n",
    "        # Update best model\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            torch.save(policy_dqn.state_dict(), best_model_path)\n",
    "        \n",
    "        \n",
    "        ############################################\n",
    "        ########### Boring Graphing Code ###########\n",
    "        ############################################\n",
    "\n",
    "        # Display progress\n",
    "        if episode % update_display_every == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Calculate moving average of rewards\n",
    "            window_size = min(100, len(rewards_per_episode))\n",
    "            avg_rewards = [np.mean(rewards_per_episode[max(0, i-window_size):i+1]) \n",
    "                          for i in range(len(rewards_per_episode))]\n",
    "            \n",
    "            # Create visualisation\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Plot rewards\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(rewards_per_episode, alpha=0.5, label='Episode Reward')\n",
    "            plt.plot(avg_rewards, label='Moving Average')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.legend()\n",
    "            plt.title(f'Episode {episode}/{num_episodes}, Best Reward: {best_reward:.1f}')\n",
    "            \n",
    "            # Plot epsilon decay\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epsilon_history)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Epsilon')\n",
    "            plt.title(f'Exploration Rate: {epsilon:.3f}, Avg Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    ############################################\n",
    "    ########### MORE Boring Graphing Code ######\n",
    "    ############################################\n",
    "    # Final update\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Training completed! Best reward: {best_reward:.1f}\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Calculate moving average of rewards\n",
    "    window_size = min(100, len(rewards_per_episode))\n",
    "    avg_rewards = [np.mean(rewards_per_episode[max(0, i-window_size):i+1]) \n",
    "                  for i in range(len(rewards_per_episode))]\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_per_episode, alpha=0.5, label='Episode Reward')\n",
    "    plt.plot(avg_rewards, label='Moving Average')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training Complete, Best Reward: {best_reward:.1f}')\n",
    "    \n",
    "    # Plot epsilon decay\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilon_history)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title(f'Final Exploration Rate: {epsilon:.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return policy_dqn\n",
    "\n",
    "# Replace the method in the Agent class\n",
    "Agent.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Functions for workshop usage\n",
    "def run_training(num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Run a short training session for demonstration purposes\n",
    "    \n",
    "    Parameters:\n",
    "    - num_episodes: Number of episodes to train (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model and agent\n",
    "    \"\"\"\n",
    "    agent = Agent()\n",
    "    model = agent.train(num_episodes=num_episodes, update_display_every=2)\n",
    "    return model, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Now let's run the training for 500 episodes!\n",
    "run_training(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_trained_agent(agent=None, model_path=\"best_flappy_model.pt\"):\n",
    "    \"\"\"\n",
    "    visualise a trained agent playing Flappy Bird\n",
    "    \n",
    "    Parameters:\n",
    "    - agent: Agent instance (creates new one if None)\n",
    "    - model_path: Path to the trained model\n",
    "    \n",
    "    Returns:\n",
    "    - Animation of the agent playing\n",
    "    \"\"\"\n",
    "    if agent is None:\n",
    "        agent = Agent()\n",
    "    return agent.run_trained_model(model_path=model_path)\n",
    "\n",
    "visualise_trained_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
