{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flappy-bird-gymnasium\n",
    "#!pip install gymnasium\n",
    "#!pip install matplotlib\n",
    "#!pip install torch\n",
    "#!pip install tqdm\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from datetime import datetime\n",
    "import time\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        Q = self.output(x)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "        \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # Which environment are we using?\n",
    "    \"env_id\": \"FlappyBird-v0\",\n",
    "    \n",
    "    # Memory size for storing past experiences\n",
    "    \"replay_memory_size\": 100000,\n",
    "    \n",
    "    # Number of experiences to sample for each learning update\n",
    "    \"mini_batch_size\": 32,\n",
    "    \n",
    "    # Initial exploration rate (1.0 = 100% random actions)\n",
    "    \"epsilon_init\": 1.0,\n",
    "    \n",
    "    # Rate at which exploration decreases\n",
    "    \"epsilon_decay\": 0.9995,\n",
    "    \n",
    "    # Minimum exploration rate (never explore less than this)\n",
    "    \"epsilon_min\": 0.05,\n",
    "    \n",
    "    # How often to update the target network (in steps)\n",
    "    \"network_sync_rate\": 10,\n",
    "    \n",
    "    # Learning rate for the neural network\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \n",
    "    # Discount factor for future rewards\n",
    "    \"discount_factor\": 0.99,\n",
    "    \n",
    "    # Training will stop if agent gets this much reward in one episode\n",
    "    \"max_reward_threshold\": 10000,\n",
    "    \n",
    "    # Neural network hidden layer size\n",
    "    \"fc1_nodes\": 256,\n",
    "    \n",
    "    # Parameters about the environment we're going to use.\n",
    "    # In our case, we just want the locations of the pipes (relative to us), \n",
    "    # and not the lidar scanning of the bird. \n",
    "    \"env_make_params\": {\"use_lidar\": False},\n",
    "    \n",
    "    # Stop training on reward threshold\n",
    "    \"stop_on_reward\": 10000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        self.env_id = hyperparameters['env_id']\n",
    "        self.learning_rate_a = hyperparameters['learning_rate']\n",
    "        self.discount_factor_g = hyperparameters['discount_factor']\n",
    "        self.network_sync_rate = hyperparameters['network_sync_rate']\n",
    "        self.replay_memory_size = hyperparameters['replay_memory_size']\n",
    "        self.mini_batch_size = hyperparameters['mini_batch_size']\n",
    "        self.epsilon_init = hyperparameters['epsilon_init']\n",
    "        self.epsilon_decay = hyperparameters['epsilon_decay']\n",
    "        self.epsilon_min = hyperparameters['epsilon_min']\n",
    "        self.stop_on_reward = hyperparameters['stop_on_reward']\n",
    "        self.fc1_nodes = hyperparameters['fc1_nodes']\n",
    "        self.env_make_params = hyperparameters.get('env_make_params', {})\n",
    "        \n",
    "        # Neural Network loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # For visualisation in notebook\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        \n",
    "    def create_environment(self, render_mode=\"rgb_array\"):\n",
    "        \"\"\"Create environment with specified render mode\"\"\"\n",
    "        return gym.make(self.env_id, render_mode=render_mode, **self.env_make_params)\n",
    "        \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        # Transpose and separate elements\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
    "        \n",
    "        # Stack tensors to create batch tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        new_states = torch.stack(new_states)\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Calculate target Q values\n",
    "            target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
    "        \n",
    "        # Calculate Q values from current policy\n",
    "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        \n",
    "        # Optimize the model (backpropagation)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, num_episodes=1000, update_display_every=5, save_path=None):\n",
    "        \"\"\"Train the agent for a specified number of episodes\"\"\"\n",
    "        \n",
    "        # Create instance of environment\n",
    "        env = self.create_environment()\n",
    "        \n",
    "        # Get state and action dimensions\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "        # Create neural networks\n",
    "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        target_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "        \n",
    "        # Initialize replay memory\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "        \n",
    "        # Initialize epsilon\n",
    "        epsilon = self.epsilon_init\n",
    "        \n",
    "        # Lists to track progress\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "        step_count = 0\n",
    "        \n",
    "        # Initialize best reward\n",
    "        best_reward = -float('inf')\n",
    "        best_model_path = save_path if save_path else \"best_flappy_model.pt\"\n",
    "        \n",
    "        # Create plot for visualisation\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Training loop\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0.0\n",
    "            episode_loss = 0.0\n",
    "            num_opt_steps = 0\n",
    "            \n",
    "            # Episode loop\n",
    "            while not (terminated or truncated) and episode_reward < self.stop_on_reward:\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # Select random action\n",
    "                    action = env.action_space.sample()\n",
    "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                else:\n",
    "                    # Select best action\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "                \n",
    "                # Execute action\n",
    "                new_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                \n",
    "                # Accumulate rewards\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Convert new state and reward to tensors\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "                \n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                \n",
    "                # Increment step counter\n",
    "                step_count += 1\n",
    "                \n",
    "                # Train if enough experiences collected\n",
    "                if len(memory) > self.mini_batch_size:\n",
    "                    mini_batch = memory.sample(self.mini_batch_size)\n",
    "                    loss = self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "                    episode_loss += loss\n",
    "                    num_opt_steps += 1\n",
    "                    \n",
    "                    # Decay epsilon\n",
    "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "                    \n",
    "                    # Sync networks\n",
    "                    if step_count > self.network_sync_rate:\n",
    "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                        step_count = 0\n",
    "                \n",
    "                # Move to next state\n",
    "                state = new_state\n",
    "            \n",
    "            # Calculate average loss for this episode\n",
    "            avg_loss = episode_loss / max(1, num_opt_steps)\n",
    "            \n",
    "            # Store epsilon at the end of episode\n",
    "            epsilon_history.append(epsilon)\n",
    "            \n",
    "            # Track rewards\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "            \n",
    "            # Update best model\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                torch.save(policy_dqn.state_dict(), best_model_path)\n",
    "            \n",
    "            # Display progress\n",
    "            if episode % update_display_every == 0:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                # Calculate moving average of rewards\n",
    "                window_size = min(100, len(rewards_per_episode))\n",
    "                avg_rewards = [np.mean(rewards_per_episode[max(0, i-window_size):i+1]) \n",
    "                              for i in range(len(rewards_per_episode))]\n",
    "                \n",
    "                # Create visualisation\n",
    "                plt.figure(figsize=(12, 5))\n",
    "                \n",
    "                # Plot rewards\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(rewards_per_episode, alpha=0.5, label='Episode Reward')\n",
    "                plt.plot(avg_rewards, label='Moving Average')\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.legend()\n",
    "                plt.title(f'Episode {episode}/{num_episodes}, Best Reward: {best_reward:.1f}')\n",
    "                \n",
    "                # Plot epsilon decay\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(epsilon_history)\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Epsilon')\n",
    "                plt.title(f'Exploration Rate: {epsilon:.3f}, Avg Loss: {avg_loss:.4f}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        # Final update\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Training completed! Best reward: {best_reward:.1f}\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Calculate moving average of rewards\n",
    "        window_size = min(100, len(rewards_per_episode))\n",
    "        avg_rewards = [np.mean(rewards_per_episode[max(0, i-window_size):i+1]) \n",
    "                      for i in range(len(rewards_per_episode))]\n",
    "        \n",
    "        # Plot rewards\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(rewards_per_episode, alpha=0.5, label='Episode Reward')\n",
    "        plt.plot(avg_rewards, label='Moving Average')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.title(f'Training Complete, Best Reward: {best_reward:.1f}')\n",
    "        \n",
    "        # Plot epsilon decay\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.title(f'Final Exploration Rate: {epsilon:.3f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return policy_dqn\n",
    "    \n",
    "    # Run a trained model and display gameplay in notebook\n",
    "    def run_trained_model(self, model_path=\"best_flappy_model.pt\", max_steps=1000, fps=30):\n",
    "        \"\"\"\n",
    "        Run a trained model and display gameplay in a Jupyter notebook\n",
    "        using matplotlib animation instead of pygame window.\n",
    "        This method is specifically designed for cloud Jupyter notebooks.\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path: Path to the saved model\n",
    "        - max_steps: Maximum number of steps to run\n",
    "        - fps: Frames per second for the display\n",
    "        \n",
    "        Returns:\n",
    "        - Total reward achieved\n",
    "        \"\"\"\n",
    "        # Setup and prepare the visualisation\n",
    "        frames = self.collect_gameplay_frames(model_path, max_steps)\n",
    "        \n",
    "        # Display animation using matplotlib animation\n",
    "        return self.display_gameplay_animation(frames)\n",
    "    \n",
    "    # Separate function to collect gameplay frames\n",
    "    def collect_gameplay_frames(self, model_path=\"best_flappy_model.pt\", max_steps=1000):\n",
    "        \"\"\"Collect frames from the agent playing the game\"\"\"\n",
    "        # Create environment with rgb_array render mode for notebook display\n",
    "        env = self.create_environment(render_mode=\"rgb_array\")\n",
    "        \n",
    "        # Get state and action dimensions\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "        # Create and load the trained model\n",
    "        policy_network = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        policy_network.load_state_dict(torch.load(model_path))\n",
    "        policy_network.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Initialize environment\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Game loop variables\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "        step = 0\n",
    "        \n",
    "        # Print progress message\n",
    "        print(\"Collecting gameplay frames... Please wait.\")\n",
    "        \n",
    "        # Collect frames while playing\n",
    "        while not (terminated or truncated) and step < max_steps:\n",
    "            # Select action based on policy\n",
    "            with torch.no_grad():\n",
    "                action = policy_network(state.unsqueeze(dim=0)).squeeze().argmax().item()\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Convert state to tensor for next iteration\n",
    "            state = torch.tensor(next_state, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Capture frame\n",
    "            frames.append(env.render())\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Show progress periodically\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Collected {step} frames, current reward: {total_reward:.1f}\")\n",
    "        \n",
    "        env.close()\n",
    "        print(f\"Game completed! Total reward: {total_reward:.1f}, collected {len(frames)} frames\")\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    # Display the collected frames as a smooth animation\n",
    "    def display_gameplay_animation(self, frames):\n",
    "        \"\"\"Display collected frames as a smooth animation in the notebook\"\"\"\n",
    "        if not frames:\n",
    "            print(\"No frames to display!\")\n",
    "            return 0\n",
    "            \n",
    "        print(f\"Creating animation with {len(frames)} frames...\")\n",
    "        \n",
    "        # Create figure for the animation\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        plt.close()  # Close the figure to prevent it from displaying twice\n",
    "        \n",
    "        # Initialize with the first frame\n",
    "        img = ax.imshow(frames[0])\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Update function for animation\n",
    "        def update(frame):\n",
    "            img.set_array(frame)\n",
    "            return [img]\n",
    "        \n",
    "        # Create the animation with a faster interval for smoother playback\n",
    "        animation = FuncAnimation(\n",
    "            fig, update, frames=frames, \n",
    "            interval=20,  # Smaller interval for smoother animation\n",
    "            blit=True\n",
    "        )\n",
    "        \n",
    "        # Display the animation in the notebook\n",
    "        return HTML(animation.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Functions for workshop usage\n",
    "def run_short_training(num_episodes=7000):\n",
    "    \"\"\"\n",
    "    Run a short training session for demonstration purposes\n",
    "    \n",
    "    Parameters:\n",
    "    - num_episodes: Number of episodes to train (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model and agent\n",
    "    \"\"\"\n",
    "    agent = Agent()\n",
    "    model = agent.train(num_episodes=num_episodes, update_display_every=2)\n",
    "    return model, agent\n",
    "\n",
    "\n",
    "def visualise_trained_agent(agent=None, model_path=\"best_flappy_model.pt\"):\n",
    "    \"\"\"\n",
    "    visualise a trained agent playing Flappy Bird\n",
    "    \n",
    "    Parameters:\n",
    "    - agent: Agent instance (creates new one if None)\n",
    "    - model_path: Path to the trained model\n",
    "    \n",
    "    Returns:\n",
    "    - Animation of the agent playing\n",
    "    \"\"\"\n",
    "    if agent is None:\n",
    "        agent = Agent()\n",
    "    return agent.run_trained_model(model_path=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_pretrained_model(github_url, local_filename=\"pretrained_model.pt\"):\n",
    "    \"\"\"\n",
    "    Download a pre-trained model from GitHub\n",
    "    \n",
    "    Parameters:\n",
    "    - github_url: URL to the model file on GitHub\n",
    "    - local_filename: Where to save the downloaded model\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the downloaded model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading pre-trained model from {github_url}...\")\n",
    "        \n",
    "        # Make HTTP request to download the file\n",
    "        response = requests.get(github_url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Get the total file size if available\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        # Download with progress bar\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            if total_size == 0:  # No content length header\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                # Use tqdm for progress bar\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "        \n",
    "        print(f\"Model successfully downloaded to {local_filename}\")\n",
    "        return local_filename\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_run_pretrained_model(model_path=\"pretrained_model.pt\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and run it in the notebook\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to the pre-trained model\n",
    "    \n",
    "    Returns:\n",
    "    - Animation of the model playing\n",
    "    \"\"\"\n",
    "    # Check if model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file {model_path} not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Create agent and run the model\n",
    "    agent = Agent()\n",
    "    return agent.run_trained_model(model_path=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
