{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGDzxfBMV7AD"
      },
      "source": [
        "# Workshop 1: Welcome to Machine Learning! üöÄ #\n",
        "\n",
        "- **When**: Monday Week 3, 17:00 - 18:30 \n",
        "- **Where**: AT 5.04\n",
        "- **Contact**: hello@edinburghai.org\n",
        "- **Credits**: This notebook is created by EdinburghAI for use in its workshops. If you plan to use it, please credit us. \n",
        "- **P.S.**: All data is FAKE!\n",
        "\n",
        "## Today\n",
        "- Use **linear regression**üìà and **decision trees**üå≤ to learn linear relationships and to classify\n",
        "- Learn about fully-connected **neural networks** üß† using Python üêç and [Pytorch](https://pytorch.org/).\n",
        "- Train your first neural net üßë‚Äçüéì\n",
        "\n",
        "Let's get started! üíØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instructions\n",
        "\n",
        "This is a Jupyter notebook. It contains cells. There are 2 kinds of cells - markdown and Python. Markdown cells are like this one, and are just there to give you information. Python cells run code. You can run a cell with `Ctrl + Enter` or run a cell and move the next one with `Shift + Enter`. Try running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Ctrl + Enter runs this cell!')\n",
        "output = 'The last line of a cell is printed by default'\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What YOU Need To Do \n",
        "\n",
        "- There are places to write code, indicated by `...` and a comment with `TODO:` in a python cell. You should fill these in or nothing will work! If you have any questions, just ask one of the EdinburghAI people :)\n",
        "\n",
        "- There are points to stop and think indicated by **Thinkü§î**. Please stop, think, maybe write an answer, and discuss with those around you. \n",
        "\n",
        "- There are also questions labelled **Extensionüòà**. These are more optional, and are for those of you who might have done some similar stuff before. Feel free to have a think about these questions though!\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Machine Learning? ü§ñ\n",
        "\n",
        "Supppose you're in charge of the Google internship hiring team. You've been tasked with creating an automated system that decides who to give internships to. You have access to their grades and their CVs. Think - how would you do it? Maybe you could write a function that assigns some score to their average grade, and adds on some extra points if they were part of programming club. But what number? And how much should you add on?\n",
        "\n",
        "Machine learning provides a way for the machine **to find this function by itself from data**. The machine chooses the function by analysing previous successful and unsuccessful intern hires, and deciding what was most important in those decisions.\n",
        "\n",
        "**Thinkü§î**: Is this a good system for deciding intern hires? Why or why not?\n",
        "\n",
        "This is all super high-level and intuititve, so let's get building to see it in action. There are loads of ways machines can learn from data. First up, we're going to cover two methods called Linear Regression and Decision Trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression #\n",
        "\n",
        "Linear regression is a fancy way of saying that you want to draw a straight line. Remember in school science class when you plotted your data and drew your line of best fit? The machine can draw this straight line for you. If you want to see how, you can google 'Ordinary Least Squares Regression' - but we'll skip the details here. \n",
        "\n",
        "**Thinkü§î**: How you would design an algorithm that draws a straight line through some points? By *drawing* a straight line, I mean deciding on what $m$ and $b$ are, in your trusty straight line equation: $y=mx + b$. Describe an algorithm to someone sitting near you!\n",
        "\n",
        "This isn't very advanced, but it is Machine Learning. Understanding what's going on here is crucial to understanding what's actually happening with more complicated machine learning methods. You give the machine some data points and a rough idea what the function should look like, and the machine decides on the detail. This is fundamentally the same as any ML algorithm. \n",
        "\n",
        "Let's see it in actionüí™. To do this, we need some data! Let's load some data about units of alcohol consumed per week and # of Big Cheese attendances per year. Each data point is a (fake) student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First we load some (FAKE) data in from a csv file using a package called pandas\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Load the data from the csv file and display the first few rows\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Next we can plot the data (as a scatterplot) using matplotlib\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can fit a linear regression model to the data. We can use the `LinearRegression` class from the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# TODO: Fit a linear regression model to the data, and plot the line of best fit with a scatterplot\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: We can look at our straight line equation using the coef_[0] and model.intercept_ attributes of our model. Fill in the blanks below! \n",
        "m = ...\n",
        "c = ...\n",
        "print(f'y = {m}x + {c}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Think and Discuss:** Why do you think `model.coef_` is a list?\n",
        "\n",
        "*Hint: Imagine you also had information on students' average bedtimes on a Saturday night and wanted to use this in your model.*\n",
        "\n",
        "Now let's predict how many big cheeses per year someone attends from their alcohol consumption using `model.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_alchol_per_week = 10\n",
        "\n",
        "# TODO: We can now predict the number of big cheeses per year for someone who drinks 10 units of alcohol per week\n",
        "prediction_big_cheeses_per_year = ...\n",
        "print(f'Predicted big cheeses per year: {round(prediction_big_cheeses_per_year, 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try messing around with the prediction and answer the following questions with those around you.\n",
        "\n",
        "What happens if you input 0 units per week? What happens if you input 50 units per week? \n",
        "\n",
        "**Thinkü§î**: Do these make sense? How many big cheese are there per year? How might you correct your model to be more realistic?\n",
        "\n",
        "*Hint: There's a big cheese every week of the academic year*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension üòà\n",
        "\n",
        "Perhaps you think this is boring üò¥ because we can only genrerate straight lines. To see why linear regression is actually more powerful than you think, try the next exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's get some new data\n",
        "big_cheese_quadratic = pd.read_csv('./data/bigcheese-quadratic.csv')\n",
        "\n",
        "# TODO: And plot it with the same basic linear model fitted to the new data\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This doesn't look as good as last time. **Thinkü§î**: How could you quantify this? How would you measure how 'good' the line is? \n",
        "\n",
        "[*Hint*](https://medium.com/@TheDataScience-ProF/demystifying-rmse-your-guide-to-understanding-root-mean-squared-error-379e41dccfd9)\n",
        "\n",
        "Maybe you can spot that this looks more like a quadratic relationship. **Thinkü§î**: Do you think you'd be able to use the same linear model to fit this relationship? If so, how?\n",
        "\n",
        "*Hint: Currently you're just passing $x$ to Linear Regression. If the relationship is quadratic, what might you pass to the Linear Regression instead?*. \n",
        "\n",
        "Try it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO: We can add a quadratic term to our model by creating a new column in our data that is the square of the 'Units of alcohol per week' column\n",
        "new_data['Units of alcohol per week squared'] = ...\n",
        "\n",
        "# TODO: And then we can fit a new model to the data with the quadratic term included\n",
        "...\n",
        "\n",
        "# And plot the data with the new model\n",
        "# Don't worry too much about the code below, it's just to make the plot look nicer\n",
        "plt.scatter(data=new_data, x='Units of alcohol per week', y='Big cheese attendances per year')\n",
        "x_smooth = np.linspace(new_data['Units of alcohol per week'].min(), new_data['Units of alcohol per week'].max(), 100)\n",
        "x_smooth_squared = x_smooth ** 2\n",
        "x_smooth_data = np.column_stack([x_smooth, x_smooth_squared])\n",
        "y_smooth = linear_model.predict(x_smooth_data)\n",
        "plt.plot(x_smooth, y_smooth, color='red', label='Quadratic fit')\n",
        "plt.xlabel('Units of alcohol per week')\n",
        "plt.ylabel('Big cheese attendances per year')\n",
        "plt.title('Big cheese attendances per year vs units of alcohol per week')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That looks a bit better! \n",
        "\n",
        "Can you see that if you messed around with whether we have an `x**2` or `x**3` term etc, or more complicated functions like `sin(x)` AND with multiple different inputs AND even with interactions between these inputs, we can actually do a lot with linear regression?\n",
        "\n",
        "Go google 'radial basis functions' or 'sigmoid basis functions' to learn more. Combined with linear regresion, it's powerful stuff!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Trees\n",
        "\n",
        "We're going to take the Google internship hiring example for this one.\n",
        "\n",
        "If you program a system by hand, you might make rules like: 'The grade must be above 70%'. Or 'If their grade is high enough, they must also have been part of programming club'. You could implement this as a bunch of if statements. But why 70%? And should it be programming club, or should you look at how many previous internships they've done? This is where Decision Trees come in. They make these decisions for you.\n",
        "\n",
        "**How it works**: On a high-level, what a decision tree does is look at the data and see which splits it can make to most neatly divide the people into who got an internship and who didn't. If you want to understand more, [here's a 4 minute video.](https://www.youtube.com/watch?v=JcI5E2Ng6r4)\n",
        "\n",
        "Let's load some internship data and have a go. Each data point is a student, with their average grade and whether or not they got hired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load some data\n",
        "data = pd.read_csv('./data/googleinternship_simple.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before running our model, let's introduce the idea of *training* and *testing*. Your model is like a student. You can give it exercises to practice, but you also want to know how good it is. So, you can give it a test with similar questions to what it's previously seen, but not exactly the same, because you don't want it just to memorise.\n",
        "\n",
        "To do this, we first split our data into train and test. We then train our model using the train data, and test our model using the *unseen* test data. We can then decide how good our model was on the test data. \n",
        "\n",
        "Let's split our data using `sklearn`'s `train_test_split`, with a ratio of 20% testing data. This number is arbitrary, but generally we test using between 10 and 40% of the data depending on how much data is available and other factors.\n",
        "\n",
        "We'll also introduce the convention of using `X` as input to the model, and `y` as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Specify the features and target variable\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# TODO: Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thinkü§î**: What is the type of `X` and `y`? How big are they? Therefore, why do you think we have a capital `X` and lower case `y`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# TODO: We can create a decision tree model\n",
        "decision_tree = ...\n",
        "\n",
        "# TODO: And fit it to the data\n",
        "...\n",
        "\n",
        "# TODO: Plot the decision tree using the plot_tree function\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(20, 15))\n",
        "...\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thinkü§î**: How do you interpret the information above? What does the `Average grade <= ...` mean? And `samples=...`? And `class = Not`? You can ignore the `gini` values - (google 'gini impurity' if you're interested). \n",
        "\n",
        "You can read the documentation on the `plot_tree` function [here.](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)\n",
        "\n",
        "Try setting the `max_depth` parameter to 2 instead of 1 and re-running the cell above. What changed? Do you think this is reasonable?\n",
        "\n",
        "How good is our tree? Let's measure its *accuracy* it on the test data!\n",
        "\n",
        "Accuracy is quite intuitive. It is the number of predicitions the model got right divided by the total number of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can use the pre-made accuracy_score function from sklearn. Alternatively, you could program it yourself quite easily.\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# TODO: We can now use our model to make predictions on the test set\n",
        "predictions = ...\n",
        "\n",
        "# TODO: We can calculate the accuracy of our model\n",
        "accuracy = ...\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow! Over 99% accuracy! \n",
        "\n",
        "**Thinkü§î**: Is this actually impressive?\n",
        "\n",
        "*Hint: What would happen if the model always guessed 'No Hire'? Could you write some code below to test what accuracy this would give? Do you even \n",
        "need to, given your decision tree above?*\n",
        "\n",
        "Let's use a different metric called *recall* which measures how many of the hires it actually detects. A recall of 1 means it successfully detected all hires, and 0 means none. \n",
        "\n",
        "**Thinkü§î**: Why this is different from accuracy? There is also a metric called *precision* - what do you think that is? \n",
        "\n",
        "**Extensionüòà**: Google 'f1-score'. What is it? Why do you think it's useful? What's a confusion matrix?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# TODO: Calculate Recall\n",
        "recall = ...\n",
        "print(f'Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh dear! Perhaps this isn't surprising given our tree above.\n",
        "\n",
        "Much of the problem here is a lack of data. We only have one column! Let's load some new data with many more columns and more data points and try again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data/googleinternship_big.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's always a good idea to try to understand the data better first. Let's look at some job hiring stats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Number of rows, number of hires, and job offer rate\n",
        "total_rows = ...\n",
        "total_hires = ...\n",
        "job_offer_rate = ...\n",
        "\n",
        "total_rows, total_hires, f'{job_offer_rate}%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now try to build a model yourself that does better. We're going to use both accuracy and recall as the main metric for grading your model here (**Thinkü§î**: Why can't we just use recall?). \n",
        "\n",
        "You can adjust the `max_depth` of your tree. This is what we call a *hyperparameter* of a model. It is not a parameter because it is not something the machine learns itself. Instead, it is something that you, as the machine learning engineer, decide on to guide the machine learning algorithm. The test ratio of 20% is another hyperparameter. Even the choice to use a decision tree is, in a way, a hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: This whole cell!\n",
        "\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# We can create a decision tree model\n",
        "decision_tree = ...\n",
        "\n",
        "# And fit it to the data\n",
        "...\n",
        "\n",
        "# We can visualise the decision tree using the export_text function this time to see which features are being used\n",
        "from sklearn.tree import export_text\n",
        "...\n",
        "\n",
        "# Make predictions and calculate recall and accuracy\n",
        "predictions = ...\n",
        "\n",
        "recall = ...\n",
        "accuracy = ...\n",
        "print(f'Recall: {recall}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully that looks a bit better.\n",
        "\n",
        "**Thinkü§î**: What were the most important features in deciding whether to hire or not? Do you like this machine learning system? Why, or why not? How would you change it to be 'better'? What does 'better mean' to you?\n",
        "\n",
        "These sorts of questions are what you need to be asking yourself every time you're building a model, and it only gets harder when the models get more complicated.\n",
        "\n",
        "Decision trees are incredibly powerful. What you've seen here is the most basic version. But they can be expanded vertically (larger depth, this is called *bagging*) and horizontally (add more trees, where each tree 'votes' on the overall outcome, this is called *boosting* or *ensembling*). If you randomly add trees together, you get a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). One of the most powerful ML techniques that isn't a neural network is called [XGBoost](https://xgboost.readthedocs.io/en/stable/) which is just bunch of fancy decision trees, where each one tries to make up for each other's weaknesses. [Here's a 4 minute video that explains how it works.](https://www.youtube.com/watch?v=TyvYZ26alZs) In many cases, XGBoost works better than neural networks, especially when you have tabular data. Decision tree are much easier to understand than a big neural networks - you can literally print them as a bunch of if statements!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Well Done!\n",
        "\n",
        "That concludes our introduction to ML! Hope you had fun! Next up, we're going to look at neural networks, which are the foundation of recent advances in AI."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMBwytVBtp6/qtJP46b97Tz",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
